{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df59f3c9-acfa-4c56-a1c9-33d95b609f26",
   "metadata": {},
   "source": [
    "# Clean folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f7b808-6b41-4593-8792-b531edc8687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "from glob import glob\n",
    "\n",
    "MAIN_PATH = \"speed_test/\"\n",
    "TEMP_PATH = f\"{MAIN_PATH}temp/\"\n",
    "RAW_PATH = f\"{MAIN_PATH}raw_data/\"\n",
    "CHECK_POINT = f\"{MAIN_PATH}_checkpoint\"\n",
    "os.makedirs(RAW_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "def clean_project():\n",
    "    try:\n",
    "        shutil.rmtree(\"spark-warehouse\")\n",
    "        shutil.rmtree(\"metastore_db\")\n",
    "        shutil.rmtree(\"speed_test/_checkpoint\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "clean_project()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b454250-bd36-49d0-bd57-ac864f964331",
   "metadata": {},
   "source": [
    "# SPEED TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efff52d7-569a-4d96-a80a-72d024f97780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_test():\n",
    "\n",
    "    def move_temp_to_raw():\n",
    "        files = glob(f\"{TEMP_PATH}*.json\")\n",
    "        list(map(lambda x: os.rename(x, x.replace(TEMP_PATH, RAW_PATH)), files))\n",
    "\n",
    "    now = dt.now()\n",
    "    file_name = f\"log_{now.strftime('%Y_%m_%d_%H_%M_%s')}.json\"\n",
    "    subprocess.run(\n",
    "        f\"touch {TEMP_PATH}{file_name} && speedtest --format=json >> {TEMP_PATH}{file_name} && mv  {TEMP_PATH}{file_name} {RAW_PATH}{file_name}\",\n",
    "        shell=True,\n",
    "        executable=\"/bin/bash\",\n",
    "    )\n",
    "    move_temp_to_raw()\n",
    "    list(\n",
    "        map(\n",
    "            os.remove,\n",
    "            filter(\n",
    "                lambda x: os.path.getsize(x) == 0, glob(\"speed_test/raw_data/*.json\")\n",
    "            ),\n",
    "        )\n",
    "        )\n",
    "    return dt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2dc1e0-95a8-4181-a73d-963ddbbfc5a9",
   "metadata": {},
   "source": [
    "# DeltaLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5af9479-a034-4dd9-9279-75102ebee48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from delta import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sql = lambda statement, limit=5: spark.sql(statement).limit(limit).toPandas()\n",
    "minutes_to_seconds = lambda x: x * 60\n",
    "run_every_n_seconds = minutes_to_seconds(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75b6352-77f9-46e2-9ebc-96d371248e2f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Delta Lake is an open source project that enables building a Lakehouse architecture on top of data lakes. Delta Lake provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing on top of existing data lakes, such as S3, ADLS, GCS, and HDFS.\n",
    "\n",
    "Specifically, Delta Lake offers:\n",
    "\n",
    "- ACID transactions on Spark: Serializable isolation levels ensure that readers never see inconsistent data.\n",
    "\n",
    "- Scalable metadata handling: Leverages Spark distributed processing power to handle all the metadata for petabyte-scale tables with billions of files at ease.\n",
    "\n",
    "- Streaming and batch unification: A table in Delta Lake is a batch table as well as a streaming source and sink. Streaming data ingest, batch historic backfill, interactive queries all just work out of the box.\n",
    "\n",
    "- Schema enforcement: Automatically handles schema variations to prevent insertion of bad records during ingestion.\n",
    "\n",
    "- Time travel: Data versioning enables rollbacks, full historical audit trails, and reproducible machine learning experiments.\n",
    "\n",
    "- Upserts and deletes: Supports merge, update and delete operations to enable complex use cases like change-data-capture, slowly-changing-dimension (SCD) operations, streaming upserts, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea5fedb-4e96-4713-b68d-3f399fa81763",
   "metadata": {},
   "source": [
    "## What Is a Lakehouse?\n",
    "Few systems are beginning to emerge that address the limitations of data lakes. A lakehouse is a new, open architecture that combines the best elements of data lakes and data warehouses. Lakehouses are enabled by a new system design: implementing similar data structures and data management features to those in a data warehouse directly on top of low cost cloud storage in open formats. They are what you would get if you had to redesign data warehouses in the modern world, now that cheap and highly reliable storage (in the form of object stores) are available.\n",
    "\n",
    "A lakehouse has the following key features:\n",
    "\n",
    "- Transaction support: In an enterprise lakehouse many data pipelines will often be reading and writing data concurrently. Support for ACID transactions ensures consistency as multiple parties concurrently read or write data, typically using SQL.\n",
    "Schema enforcement and governance: The Lakehouse should have a way to support schema enforcement and evolution, supporting DW schema architectures such as star/snowflake-schemas. The system should be able to reason about data integrity, and it should have robust governance and auditing mechanisms.\n",
    "- BI support: Lakehouses enable using BI tools directly on the source data. This reduces staleness and improves recency, reduces latency, and lowers the cost of having to operationalize two copies of the data in both a data lake and a warehouse.\n",
    "Storage is decoupled from compute: In practice this means storage and compute use separate clusters, thus these systems are able to scale to many more concurrent users and larger data sizes. Some modern data warehouses also have this property.\n",
    "- Openness: The storage formats they use are open and standardized, such as Parquet, and they provide an API so a variety of tools and engines, including machine learning and Python/R libraries, can efficiently access the data directly.\n",
    "- Support for diverse data types ranging from unstructured to structured data: The lakehouse can be used to store, refine, analyze, and access data types needed for many new data applications, including images, video, audio, semi-structured data, and text.\n",
    "- Support for diverse workloads: including data science, machine learning, and SQL and analytics. Multiple tools might be needed to support all these workloads but they all rely on the same data repository.\n",
    "- End-to-end streaming: Real-time reports are the norm in many enterprises. Support for streaming eliminates the need for separate systems dedicated to serving real-time data applications.\n",
    "\n",
    "<center> <img src='img/warehouseVSlakeVslakehouse.png'> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f5fbc-cb60-4ac2-9f3f-e02717bb6cb4",
   "metadata": {},
   "source": [
    "## Medallion Arquitecture (Multi Hopp):\n",
    "\n",
    "What is a medallion architecture?\n",
    "A medallion architecture is a data design pattern used to logically organize data in a lakehouse, with the goal of incrementally and progressively improving the structure and quality of data as it flows through each layer of the architecture (from Bronze ⇒ Silver ⇒ Gold layer tables). Medallion architectures are sometimes also referred to as \"multi-hop\" architectures.\n",
    "\n",
    "<center> <img src='img/multihop.png'> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89776d19-cf65-42d7-9c26-19e104fc6e04",
   "metadata": {},
   "source": [
    "### Bronze layer (raw data)\r\n",
    "The Bronze layer is where we land all the data from external source systems. The table structures in this layer correspond to the source system table structures \"as-is,\" along with any additional metadata columns that capture the load date/time, process ID, etc. The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7437109-544a-4ee7-96df-d06ac7b55835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/15 21:06:17 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/15 21:06:17 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/15 21:06:20 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "24/04/15 21:06:20 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore hadoop@127.0.1.1\n",
      "24/04/15 21:06:20 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "24/04/15 21:06:20 WARN ObjectStore: Failed to get database speed_test, returning NoSuchObjectException\n",
      "24/04/15 21:06:20 WARN ObjectStore: Failed to get database speed_test, returning NoSuchObjectException\n",
      "24/04/15 21:06:20 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "24/04/15 21:06:20 WARN ObjectStore: Failed to get database speed_test, returning NoSuchObjectException\n",
      "24/04/15 21:06:22 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/04/15 21:06:24 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`speed_test`.`speed_test_logs` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/04/15 21:06:24 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/04/15 21:06:24 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/04/15 21:06:24 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/15 21:06:24 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/15 21:06:25 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`speed_test`.`silver_speed_test_logs` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCHEMA = \"speed_test\"\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA} \")\n",
    "\n",
    "spark.sql(\n",
    "    \"set spark.databricks.delta.changeDataFeed.timestampOutOfRange.enabled = true;\"\n",
    ")\n",
    "spark.sql(\"set SQLConf.ADAPTIVE_EXECUTION_ENABLED.key= true\")\n",
    "\n",
    "BRONZE_TABLE = f\"{SCHEMA}.speed_test_logs\"\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "  CREATE  TABLE IF NOT EXISTS {BRONZE_TABLE} \n",
    "  (`timestamp` Timestamp\n",
    "  ) USING delta\n",
    "  TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "SILVER_TABLE = f'{SCHEMA}.silver_speed_test_logs'\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "  CREATE  TABLE IF NOT EXISTS {SILVER_TABLE } \n",
    "  (`timestamp` Timestamp\n",
    "  ) USING delta\n",
    "  TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d2c36a5-3ff3-4347-9726-48c2c9add3cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "      <th>engineInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2024-04-15 21:06:21.458</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CREATE TABLE</td>\n",
       "      <td>{'description': None, 'partitionBy': '[]', 'pr...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>True</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.5.1 Delta-Lake/3.1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName     operation  \\\n",
       "0        0 2024-04-15 21:06:21.458   None     None  CREATE TABLE   \n",
       "\n",
       "                                 operationParameters   job notebook clusterId  \\\n",
       "0  {'description': None, 'partitionBy': '[]', 'pr...  None     None      None   \n",
       "\n",
       "   readVersion isolationLevel  isBlindAppend operationMetrics userMetadata  \\\n",
       "0          NaN   Serializable           True               {}         None   \n",
       "\n",
       "                            engineInfo  \n",
       "0  Apache-Spark/3.5.1 Delta-Lake/3.1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(f\"DESCRIBE HISTORY  {BRONZE_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab497731-49b1-462e-aa44-8c0de363e6b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [timestamp]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(f\"SELECT * FROM {BRONZE_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bd13f1b-f12a-496a-b26e-db0895a47699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_bronze_table():\n",
    "\n",
    "    streaming_logs_schema = StructType(\n",
    "        [\n",
    "            StructField(\n",
    "                \"download\",\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"bandwidth\", LongType(), True),\n",
    "                        StructField(\"bytes\", LongType(), True),\n",
    "                        StructField(\"elapse\", LongType(), True),\n",
    "                        StructField(\"latency\", StringType(), True),\n",
    "                    ]\n",
    "                ),\n",
    "                True,\n",
    "            ),\n",
    "            StructField(\n",
    "                \"ping\",\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"jitter\", FloatType(), True),\n",
    "                        StructField(\"latency\", FloatType(), True),\n",
    "                        StructField(\"low\", FloatType(), True),\n",
    "                        StructField(\"high\", FloatType(), True),\n",
    "                    ]\n",
    "                ),\n",
    "                True,\n",
    "            ),\n",
    "            StructField(\"isp\", StringType(), True),\n",
    "            StructField(\n",
    "                \"result\",\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"id\", StringType(), True),\n",
    "                        StructField(\"url\", StringType(), True),\n",
    "                        StructField(\"persisted\", BooleanType(), True),\n",
    "                    ]\n",
    "                ),\n",
    "                True,\n",
    "            ),\n",
    "            StructField(\"server\", StructType([ \n",
    "                    StructField(\"id\", StringType(), True),\n",
    "                        StructField(\"host\", StringType(), True),\n",
    "                        StructField(\"port\", LongType(), True),\n",
    "                        StructField('name',StringType(),True),\n",
    "                        StructField('location',StringType(),True),\n",
    "                        StructField('country',StringType(),True),\n",
    "                        StructField('ip',StringType(),True)]),True),\n",
    "            StructField(\"timestamp\", TimestampType(), True),\n",
    "            StructField(\"type\", StringType(), True),\n",
    "            StructField(\n",
    "                \"upload\",\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"bandwidth\", LongType(), True),\n",
    "                        StructField(\"bytes\", LongType(), True),\n",
    "                        StructField(\"elapse\", LongType(), True),\n",
    "                        StructField(\"latency\", StringType(), True),\n",
    "                    ]\n",
    "                ),\n",
    "                True,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    streaming_logs_sdf = (\n",
    "        spark.readStream.option(\"ignoreCorruptFiles\", \"true\")\n",
    "        .schema(streaming_logs_schema)\n",
    "        .json(RAW_PATH)\n",
    "        .where(F.col(\"timestamp\").isNotNull())\n",
    "        .withColumn(\"log_file\", F.input_file_name())\n",
    "    )\n",
    "\n",
    "    streaming_logs_sdf.writeStream.format(\"delta\").outputMode(\"append\").option(\n",
    "        \"checkpointLocation\", CHECK_POINT\n",
    "    ).trigger(availableNow=True).option(\"overwriteSchema\", \"true\").option(\n",
    "        \"mergeSchema\", \"true\"\n",
    "    ).toTable(\n",
    "        BRONZE_TABLE\n",
    "    ).awaitTermination()\n",
    "    print('BRONZE UPDATED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e278f754-21d9-4f72-9324-6318baaf5f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/15 21:52:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/04/15 21:52:14 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/04/15 21:52:14 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/15 21:52:14 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "speed_test()\n",
    "update_bronze_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77b24257-e7f5-4a00-8927-f893479a7acf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>download</th>\n",
       "      <th>ping</th>\n",
       "      <th>isp</th>\n",
       "      <th>result</th>\n",
       "      <th>server</th>\n",
       "      <th>type</th>\n",
       "      <th>upload</th>\n",
       "      <th>log_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-04-15 21:07:36</td>\n",
       "      <td>(62536717, 702644896, None, {\"iqm\":34.165,\"low...</td>\n",
       "      <td>(4.410999774932861, 12.133000373840332, 5.0370...</td>\n",
       "      <td>Tigo Panama</td>\n",
       "      <td>(f53576d3-0ef1-4ad7-8b32-d9d1563387a7, https:/...</td>\n",
       "      <td>(31830, speedtest.ufinet.com.pa, 8080, Ufinet,...</td>\n",
       "      <td>result</td>\n",
       "      <td>(1855289, 6678176, None, {\"iqm\":7.396,\"low\":5....</td>\n",
       "      <td>file:///home/hadoop/deltalake/speed_test/raw_d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-04-15 21:06:39</td>\n",
       "      <td>(59707854, 402856768, None, {\"iqm\":35.229,\"low...</td>\n",
       "      <td>(4.589000225067139, 12.3100004196167, 11.20400...</td>\n",
       "      <td>Tigo Panama</td>\n",
       "      <td>(9e00ef59-e13c-40c3-afc0-20345fe7edcb, https:/...</td>\n",
       "      <td>(28116, speed1.transoceanet.com, 8080, Trans O...</td>\n",
       "      <td>result</td>\n",
       "      <td>(1852077, 7229864, None, {\"iqm\":10.868,\"low\":5...</td>\n",
       "      <td>file:///home/hadoop/deltalake/speed_test/raw_d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-04-15 21:05:23</td>\n",
       "      <td>(64217079, 918590928, None, {\"iqm\":62.217,\"low...</td>\n",
       "      <td>(3.1600000858306885, 12.675000190734863, 10.70...</td>\n",
       "      <td>Tigo Panama</td>\n",
       "      <td>(262c8820-faed-4457-b4fe-382ccb1ffa7f, https:/...</td>\n",
       "      <td>(31830, speedtest.ufinet.com.pa, 8080, Ufinet,...</td>\n",
       "      <td>result</td>\n",
       "      <td>(1835840, 6802704, None, {\"iqm\":7.065,\"low\":5....</td>\n",
       "      <td>file:///home/hadoop/deltalake/speed_test/raw_d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-15 19:33:25</td>\n",
       "      <td>(52752789, 701208480, None, {\"iqm\":25.712,\"low...</td>\n",
       "      <td>(2.9070000648498535, 11.720999717712402, 10.47...</td>\n",
       "      <td>Tigo Panama</td>\n",
       "      <td>(a37a478c-5ec3-450d-9d4d-5e17ddfe5e1f, https:/...</td>\n",
       "      <td>(11318, speedtest.shadwell.com.pa, 8080, Shadw...</td>\n",
       "      <td>result</td>\n",
       "      <td>(1849241, 7630960, None, {\"iqm\":12.642,\"low\":4...</td>\n",
       "      <td>file:///home/hadoop/deltalake/speed_test/raw_d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-04-15 19:32:49</td>\n",
       "      <td>(55521283, 511020920, None, {\"iqm\":32.806,\"low...</td>\n",
       "      <td>(4.864999771118164, 10.866000175476074, 6.9739...</td>\n",
       "      <td>Tigo Panama</td>\n",
       "      <td>(f7693fad-e747-4d87-b444-3bfd403d2c36, https:/...</td>\n",
       "      <td>(40265, velocidad.pacificnetcom.com, 8080, Pac...</td>\n",
       "      <td>result</td>\n",
       "      <td>(1859380, 7835128, None, {\"iqm\":7.726,\"low\":5....</td>\n",
       "      <td>file:///home/hadoop/deltalake/speed_test/raw_d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp                                           download  \\\n",
       "0 2024-04-15 21:07:36  (62536717, 702644896, None, {\"iqm\":34.165,\"low...   \n",
       "1 2024-04-15 21:06:39  (59707854, 402856768, None, {\"iqm\":35.229,\"low...   \n",
       "2 2024-04-15 21:05:23  (64217079, 918590928, None, {\"iqm\":62.217,\"low...   \n",
       "3 2024-04-15 19:33:25  (52752789, 701208480, None, {\"iqm\":25.712,\"low...   \n",
       "4 2024-04-15 19:32:49  (55521283, 511020920, None, {\"iqm\":32.806,\"low...   \n",
       "\n",
       "                                                ping          isp  \\\n",
       "0  (4.410999774932861, 12.133000373840332, 5.0370...  Tigo Panama   \n",
       "1  (4.589000225067139, 12.3100004196167, 11.20400...  Tigo Panama   \n",
       "2  (3.1600000858306885, 12.675000190734863, 10.70...  Tigo Panama   \n",
       "3  (2.9070000648498535, 11.720999717712402, 10.47...  Tigo Panama   \n",
       "4  (4.864999771118164, 10.866000175476074, 6.9739...  Tigo Panama   \n",
       "\n",
       "                                              result  \\\n",
       "0  (f53576d3-0ef1-4ad7-8b32-d9d1563387a7, https:/...   \n",
       "1  (9e00ef59-e13c-40c3-afc0-20345fe7edcb, https:/...   \n",
       "2  (262c8820-faed-4457-b4fe-382ccb1ffa7f, https:/...   \n",
       "3  (a37a478c-5ec3-450d-9d4d-5e17ddfe5e1f, https:/...   \n",
       "4  (f7693fad-e747-4d87-b444-3bfd403d2c36, https:/...   \n",
       "\n",
       "                                              server    type  \\\n",
       "0  (31830, speedtest.ufinet.com.pa, 8080, Ufinet,...  result   \n",
       "1  (28116, speed1.transoceanet.com, 8080, Trans O...  result   \n",
       "2  (31830, speedtest.ufinet.com.pa, 8080, Ufinet,...  result   \n",
       "3  (11318, speedtest.shadwell.com.pa, 8080, Shadw...  result   \n",
       "4  (40265, velocidad.pacificnetcom.com, 8080, Pac...  result   \n",
       "\n",
       "                                              upload  \\\n",
       "0  (1855289, 6678176, None, {\"iqm\":7.396,\"low\":5....   \n",
       "1  (1852077, 7229864, None, {\"iqm\":10.868,\"low\":5...   \n",
       "2  (1835840, 6802704, None, {\"iqm\":7.065,\"low\":5....   \n",
       "3  (1849241, 7630960, None, {\"iqm\":12.642,\"low\":4...   \n",
       "4  (1859380, 7835128, None, {\"iqm\":7.726,\"low\":5....   \n",
       "\n",
       "                                            log_file  \n",
       "0  file:///home/hadoop/deltalake/speed_test/raw_d...  \n",
       "1  file:///home/hadoop/deltalake/speed_test/raw_d...  \n",
       "2  file:///home/hadoop/deltalake/speed_test/raw_d...  \n",
       "3  file:///home/hadoop/deltalake/speed_test/raw_d...  \n",
       "4  file:///home/hadoop/deltalake/speed_test/raw_d...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(f \"\"\" SELECT * FROM {BRONZE_TABLE} ORDER BY timestamp DESC LIMIT 5 \"\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29622ef-08fb-41f2-be17-a4d2a3dd3c9c",
   "metadata": {},
   "source": [
    "### Silver layer (cleansed and conformed data)\n",
    "In the Silver layer of the lakehouse, the data from the Bronze layer is matched, merged, conformed and cleansed (\"just-enough\") so that the Silver layer can provide an \"Enterprise view\" of all its key business entities, concepts and transactions. (e.g. master customers, stores, non-duplicated transactions and cross-reference tables).\n",
    "\n",
    "The Silver layer brings the data from different sources into an Enterprise view and enables self-service analytics for ad-hoc reporting, advanced analytics and ML. It serves as a source for Departmental Analysts, Data Engineers and Data Scientists to further create projects and analysis to answer business problems via enterprise and departmental data projects in the Gold Layer.\n",
    "\n",
    "In the lakehouse data engineering paradigm, typically the ELT methodology is followed vs. ETL - which means only minimal or \"just-enough\" transformations and data cleansing rules are applied while loading the Silver layer. Speed and agility to ingest and deliver the data in the data lake is prioritized, and a lot of project-specific complex transformations and business rules are applied while loading the data from the Silver to Gold layer. From a data modeling perspective, the Silver Layer has more 3rd-Normal Form like data models. Data Vault-like, write-performant data models can be used in this layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc38d9f-4f77-445a-9084-d9dcd5607a39",
   "metadata": {},
   "source": [
    "#### Change data feed\n",
    "Change Data Feed (CDF) feature allows Delta tables to track row-level changes between versions of a Delta table. When enabled on a Delta table, the runtime records “change events” for all the data written into the table. This includes the row data along with metadata indicating whether the specified row was inserted, deleted, or updated.\n",
    "\n",
    "You can read the change events in batch queries using DataFrame APIs (that is, df.read) and in streaming queries using DataFrame APIs (that is, df.readStream).\n",
    "\n",
    "Use cases\n",
    "Change Data Feed is not enabled by default. The following use cases should drive when you enable the change data feed.\n",
    "\n",
    "Silver and Gold tables: Improve Delta performance by processing only row-level changes following initial MERGE, UPDATE, or DELETE operations to accelerate and simplify ETL and ELT operations.\n",
    "\n",
    "Transmit changes: Send a change data feed to downstream systems such as Kafka or RDBMS that can use it to incrementally process in later stages of data pipelines.\n",
    "\n",
    "Audit trail table: Capture the change data feed as a Delta table provides perpetual storage and efficient query capability to see all changes over time, including when deletes occur and what updates were made.\n",
    "\n",
    "\n",
    "| Column name | Type | Values |  \n",
    "|-------------|----------|------------------------|\n",
    "| _change_type  | String  | insert, update_preimage , update_postimage, delete  | \n",
    "| _commit_version | Long  | The Delta log or table version containing the change. | \n",
    "| _commit_timestamp | Timestamp |The timestamp associated when the commit was created. |\n",
    "\n",
    "\n",
    "### Use cases\n",
    "Change Data Feed is not enabled by default. The following use cases should drive when you enable the change data feed.\n",
    "\n",
    "Silver and Gold tables: Improve Delta performance by processing only row-level changes following initial MERGE, UPDATE, or DELETE operations to accelerate and simplify ETL and ELT operations.\n",
    "\n",
    "Transmit changes: Send a change data feed to downstream systems such as Kafka or RDBMS that can use it to incrementally process in later stages of data pipelines.\n",
    "\n",
    "Audit trail table: Capture the change data feed as a Delta table provides perpetual storage and efficient query capability to see all changes over time, including when deletes occur and what updates were \n",
    "\n",
    "### Enable change data feed\n",
    "\n",
    "- New table: Set the table property delta.enableChangeDataFeed = true in the CREATE TABLE command:<br>```CREATE TABLE student (id INT, name STRING, age INT) TBLPROPERTIES (delta.enableChangeDataFeed = true)```\n",
    "  \n",
    "- Existing table: Set the table property delta.enableChangeDataFeed = true in the ALTER TABLE command.<br>```ALTER TABLE myDeltaTable SET TBLPROPERTIES (delta.enableChangeDataFeed = true)```\n",
    "\n",
    "- All new tables:<br>\n",
    "      ```set spark.databricks.delta.properties.defaults.enableChangeDataFeed = true```made.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "67b5242a-b0ad-4db4-91e9-fd5974a2d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_silver_sdf():\n",
    "    bronze_delta_table = DeltaTable.forName(spark,BRONZE_TABLE)\n",
    "    silver_delta_table = DeltaTable.forName(spark,SILVER_TABLE)\n",
    "    def clean_bronze_table(sdf):\n",
    "        sdf = (\n",
    "        sdf\n",
    "        .select(\n",
    "            [\n",
    "                F.col(\"result\").getItem(\"id\").alias(\"test_id\"),\n",
    "                F.col(\"timestamp\"),\n",
    "                F.to_date(\"timestamp\").alias(\"date\"),\n",
    "                F.hour(\"timestamp\").alias(\"hour\"),\n",
    "                \"download\",\n",
    "                \"upload\",\n",
    "                'isp',\n",
    "               F.col('server').getItem('name').alias('server_name'),\n",
    "                F.col('server').getItem('ip').alias('server_ip'),\n",
    "                F.col('server').getItem('location').alias('server_location'),\n",
    "                F.col('server').getItem('country').alias('server_country')\n",
    "            ]\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"part_of_the_day\",\n",
    "            (\n",
    "                F.when((F.col(\"hour\") >= 5) & (F.col(\"hour\") < 12), \"Morning\")\n",
    "                .when((F.col(\"hour\") >= 12) & (F.col(\"hour\") < 17), \"Afternoon\")\n",
    "                .when((F.col(\"hour\") >= 17) & (F.col(\"hour\") < 21), \"Evening\")\n",
    "                .otherwise(\"Night\")\n",
    "            ),\n",
    "        )\n",
    "        .withColumn(\"download_Mbytes\", F.col(\"download\").getItem(\"bytes\") / 1000000)\n",
    "        .withColumn(\"upload_Mbytes\", F.col(\"upload\").getItem(\"bytes\") / 1000000)\n",
    "        .drop('download')\n",
    "        .drop('upload')\n",
    "            )\n",
    "        return sdf\n",
    "        \n",
    "    if silver_delta_table.toDF().limit(1).count() == 0:\n",
    "        \n",
    "        \n",
    "        sdf = clean_bronze_table(spark.read.format('delta').table(BRONZE_TABLE))\n",
    "        (sdf.write.format('delta').mode(\"append\").option(\n",
    "        \"mergeSchema\", \"true\").saveAsTable(SILVER_TABLE)\n",
    "        )\n",
    "        print('SILVER FIRST LOAD')\n",
    "    else: \n",
    "        bronze_last_update = bronze_delta_table.history().where('operation != \"CREATE TABLE\"').select(F.max('timestamp').alias('bronze_last_update')).collect()[0][0]\n",
    "        silver_last_update = silver_delta_table.history().where('operation != \"CREATE TABLE\"').select(F.max('timestamp').alias('bronze_last_update')).collect()[0][0]\n",
    "        if bronze_last_update > silver_last_update:\n",
    "            sdf = clean_bronze_table(spark.read.format('delta')\n",
    "                .option(\"readChangeFeed\", \"true\")\\\n",
    "                    .option(\"startingTimestamp\", str(silver_last_update))\n",
    "                  .table(BRONZE_TABLE))\n",
    "            silver_delta_table.alias('sink').merge(sdf.alias('source'),'source.test_id = sink.test_id').whenNotMatchedInsertAll().execute()\n",
    "            print('SILVER UPDATED')\n",
    "        else:\n",
    "            print('No updates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4cf9307e-19a0-47e6-9690-4c91c6241f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/15 21:56:33 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/04/15 21:56:34 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/04/15 21:56:34 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/15 21:56:34 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SILVER UPDATED\n"
     ]
    }
   ],
   "source": [
    "speed_test()\n",
    "update_bronze_table()\n",
    "update_silver_sdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e7c63fea-c1a2-4931-995d-9341a1e670e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'numOutputRows': '1',\n",
       " 'numTargetBytesAdded': '3931',\n",
       " 'numTargetRowsInserted': '1',\n",
       " 'numTargetFilesAdded': '1',\n",
       " 'numTargetRowsMatchedDeleted': '0',\n",
       " 'numTargetFilesRemoved': '0',\n",
       " 'numTargetRowsMatchedUpdated': '0',\n",
       " 'executionTimeMs': '524',\n",
       " 'numTargetDeletionVectorsUpdated': '0',\n",
       " 'numTargetRowsCopied': '0',\n",
       " 'rewriteTimeMs': '523',\n",
       " 'numTargetRowsUpdated': '0',\n",
       " 'numTargetDeletionVectorsRemoved': '0',\n",
       " 'numTargetRowsDeleted': '0',\n",
       " 'scanTimeMs': '0',\n",
       " 'numSourceRows': '1',\n",
       " 'numTargetDeletionVectorsAdded': '0',\n",
       " 'numTargetChangeFilesAdded': '0',\n",
       " 'numTargetRowsNotMatchedBySourceUpdated': '0',\n",
       " 'numTargetRowsNotMatchedBySourceDeleted': '0',\n",
       " 'numTargetBytesRemoved': '0'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(f'DESCRIBE HISTORY {SILVER_TABLE}').operationMetrics.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7863f92-0b41-424a-a2a1-36c6da7f62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bgcolor = \"#2c292d\"\n",
    "# paper_bgcolor =\"#211f22\"\n",
    "paper_bgcolor = \"#1a1d21\"\n",
    "download_color = \"#ab9df2\"\n",
    "upload_color = \"#78dce8\"\n",
    "default_fontcolor = \"white\"\n",
    "\n",
    "import dash\n",
    "import dash_bootstrap_components as dbc\n",
    "import plotly.graph_objects as go\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "def line_chart_download_vs_upload(fig, df):\n",
    "\n",
    "    x = df.log_timestamp\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x, y=df.upload_Mbytes, name=\"Upload (Mbps)\", line=dict(color=upload_color)\n",
    "        ),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"<b>Upload (Mbps)</b>\",\n",
    "        color=upload_color,\n",
    "        rangemode=\"tozero\",\n",
    "        showgrid=False,\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=df.download_Mbytes,\n",
    "            name=\"Download (Mbps)\",\n",
    "            line=dict(color=download_color),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"<b>Download (Mbps)</b>\",\n",
    "        color=download_color,\n",
    "        rangemode=\"tozero\",\n",
    "        showgrid=False,\n",
    "        row=2,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(\n",
    "        showgrid=False,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor=plot_bgcolor,\n",
    "        paper_bgcolor=paper_bgcolor,\n",
    "        font=dict(color=\"white\"),\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.15, xanchor=\"right\", x=1),\n",
    "    )\n",
    "\n",
    "\n",
    "def gauges_indicators(fig, value):\n",
    "\n",
    "    def gauge_chart(value, steps, title, color):\n",
    "        max_step = steps[-1][-1]\n",
    "        title = f\"{title} <span style='font-size:0.8em;color:gray'>MBps</span><br><span style='font-size:0.5em;color:gray'>Average</span>\"\n",
    "        gauge = go.Indicator(\n",
    "            mode=\"gauge+number+delta\",\n",
    "            value=value,\n",
    "            domain={\"x\": [0.25, 0.55], \"y\": [0.25, 0.55]},\n",
    "            title={\"text\": title, \"font\": {\"size\": 25}, \"align\": \"center\"},\n",
    "            delta={\n",
    "                \"reference\": steps[-1][0],\n",
    "                \"font\": {\"size\": 13},\n",
    "                \"increasing\": {\"color\": color},\n",
    "            },\n",
    "            number={\"font\": {\"size\": 25}},\n",
    "            gauge={\n",
    "                \"axis\": {\n",
    "                    \"range\": [None, max_step],\n",
    "                    \"tickwidth\": 2,\n",
    "                    \"tickcolor\": plot_bgcolor,\n",
    "                },\n",
    "                \"bar\": {\"color\": color},\n",
    "                \"bgcolor\": \"white\",\n",
    "                \"borderwidth\": 2,\n",
    "                \"bordercolor\": \"gray\",\n",
    "                \"steps\": [\n",
    "                    {\"range\": steps[0], \"color\": \"#ff6188\"},\n",
    "                    {\"range\": steps[1], \"color\": \"#fc9867\"},\n",
    "                    {\"range\": steps[2], \"color\": \"#a9dc76\"},\n",
    "                ],\n",
    "            },\n",
    "        )\n",
    "\n",
    "        return gauge\n",
    "\n",
    "    fig.add_trace(\n",
    "        gauge_chart(\n",
    "            value[\"upload_Mbytes\"],\n",
    "            steps=[[0, 10], [10, 15], [15, 20]],\n",
    "            title=f\"<span style='font-size:0.8em;color:{upload_color}'>Upload</span>\",\n",
    "            color=upload_color,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        gauge_chart(\n",
    "            value[\"download_Mbytes\"],\n",
    "            steps=[[0, 450], [450, 600], [600, 700]],\n",
    "            title=f\"<span style='font-size:0.8em;color:{download_color}'>Download</span>\",\n",
    "            color=download_color,\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        paper_bgcolor=paper_bgcolor,\n",
    "        font={\"color\": \"white\", \"family\": \"Arial\"},\n",
    "        showlegend=False,\n",
    "    )\n",
    "    fig.update_traces(number=dict(font=dict(size=28)), delta=dict(font=dict(size=25)))\n",
    "\n",
    "\n",
    "def multiplot_speedtest(df):\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        specs=[[{\"type\": \"domain\"}, {}], [{\"type\": \"domain\"}, {}]],\n",
    "        column_widths=[0.30, 0.70],\n",
    "        row_heights=[0.25, 0.25],\n",
    "        horizontal_spacing=0.15,\n",
    "        vertical_spacing=0.15,\n",
    "    )\n",
    "\n",
    "    values = df.iloc[-3:].mean()\n",
    "    gauges_indicators(fig, values)\n",
    "    line_chart_download_vs_upload(fig, df)\n",
    "    fig.update_layout(height=550, margin=dict(l=35, r=35, b=30, t=55))\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def register_Callback(app):\n",
    "    @app.callback(\n",
    "        Output(\"stream_line_chart\", \"figure\"),\n",
    "        [\n",
    "            Input(\"interval-component\", \"n_intervals\"),\n",
    "        ],\n",
    "    )\n",
    "    def streamFig(intervals):\n",
    "        df = (\n",
    "            spark.read.table(BRONZE_TABLE)\n",
    "            .where(\n",
    "                F.col(\"log_timestamp\")\n",
    "                >= (F.current_timestamp() - F.expr(\"INTERVAL 60 minutes\"))\n",
    "            )\n",
    "            .orderBy(\"log_timestamp\", ascending=False)\n",
    "            .limit(10)\n",
    "            .toPandas()\n",
    "            .sort_values(\"log_timestamp\", ascending=True)\n",
    "        )\n",
    "        return multiplot_speedtest(df)\n",
    "\n",
    "\n",
    "config = {\"displaylogo\": False, \"scrollZoom\": False, \"displayModeBar\": False}\n",
    "\n",
    "updates = dcc.Interval(\n",
    "    id=\"interval-component\", interval=10000, n_intervals=0  # in milliseconds\n",
    ")\n",
    "\n",
    "\n",
    "navbar = dbc.Navbar(\n",
    "    dbc.Container(\n",
    "        [\n",
    "            html.A(\n",
    "                # Use row and col to control vertical alignment of logo / brand\n",
    "                dbc.Row(\n",
    "                    [\n",
    "                        dbc.Col(\n",
    "                            html.Img(\n",
    "                                src=\"https://www.pinclipart.com/picdir/big/491-4917274_panama-flag-png-palestine-flag-vector-clipart.png\",\n",
    "                                height=\"30px\",\n",
    "                            )\n",
    "                        ),\n",
    "                        dbc.Col(\n",
    "                            dbc.NavbarBrand(\n",
    "                                \"Network Speed Test by Jose Quesada\", className=\"ms-2\"\n",
    "                            )\n",
    "                        ),\n",
    "                    ],\n",
    "                    align=\"center\",\n",
    "                    className=\"g-0\",\n",
    "                ),\n",
    "                href=\"https://plotly.com\",\n",
    "                style={\"textDecoration\": \"none\"},\n",
    "            ),\n",
    "            dbc.NavbarToggler(id=\"navbar-toggler\", n_clicks=0),\n",
    "        ]\n",
    "    ),\n",
    "    color=paper_bgcolor,\n",
    "    dark=True,\n",
    ")\n",
    "\n",
    "\n",
    "streaming_col = dbc.Col(dcc.Graph(id=\"stream_line_chart\", config=config))\n",
    "\n",
    "layout = dbc.Container(\n",
    "    [\n",
    "        navbar,\n",
    "        dbc.Container(\n",
    "            [\n",
    "                updates,\n",
    "                dcc.Store(id=\"last_32hrs\"),\n",
    "                dbc.Row(streaming_col),\n",
    "            ],\n",
    "            style={\"background-color\": paper_bgcolor, \"color\": default_fontcolor},\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "app = dash.Dash(\n",
    "    external_stylesheets=[\n",
    "        \"https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css\"\n",
    "    ],\n",
    ")\n",
    "# app.config.suppress_callback_exceptions = True\n",
    "app.layout = layout\n",
    "register_Callback(app)\n",
    "app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66d0d526-356c-4cca-868f-13cb6d947182",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/14 20:32:46 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/14 20:32:46 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/14 20:32:46 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/04/14 20:32:56 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/14 20:32:56 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/14 20:33:07 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/14 20:33:07 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3299640b-ae30-4892-aafd-213ce03a0273",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTableLogs = DeltaTable.forName(spark, BRONZE_TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e51e432-24b9-4137-8420-3cd5ea09e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "windowedCounts = (\n",
    "    words.withWatermark(\"timestamp\", \"10 minutes\")\n",
    "    .groupBy(window(words.timestamp, \"10 minutes\", \"5 minutes\"), words.word)\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9dac60-3363-454e-b15d-327604353e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUpdates = deltaTablePeopleUpdates.toDF()\n",
    "\n",
    "deltaTablePeople.alias(\"people\").merge(\n",
    "    dfUpdates.alias(\"updates\"), \"people.id = updates.id\"\n",
    ").whenMatchedUpdate(\n",
    "    set={\n",
    "        \"id\": \"updates.id\",\n",
    "        \"firstName\": \"updates.firstName\",\n",
    "        \"middleName\": \"updates.middleName\",\n",
    "        \"lastName\": \"updates.lastName\",\n",
    "        \"gender\": \"updates.gender\",\n",
    "        \"birthDate\": \"updates.birthDate\",\n",
    "        \"ssn\": \"updates.ssn\",\n",
    "        \"salary\": \"updates.salary\",\n",
    "    }\n",
    ").whenNotMatchedInsert(\n",
    "    values={\n",
    "        \"id\": \"updates.id\",\n",
    "        \"firstName\": \"updates.firstName\",\n",
    "        \"middleName\": \"updates.middleName\",\n",
    "        \"lastName\": \"updates.lastName\",\n",
    "        \"gender\": \"updates.gender\",\n",
    "        \"birthDate\": \"updates.birthDate\",\n",
    "        \"ssn\": \"updates.ssn\",\n",
    "        \"salary\": \"updates.salary\",\n",
    "    }\n",
    ").execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
