{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af2dc1e0-95a8-4181-a73d-963ddbbfc5a9",
   "metadata": {},
   "source": [
    "# DeltaLake\n",
    "- [DeltaLake Getting Started](https://delta.io/learn/getting-started)\n",
    "- [DeltaLake Best Practics](https://docs.delta.io/latest/best-practices.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75b6352-77f9-46e2-9ebc-96d371248e2f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Delta Lake is an open source project that enables building a Lakehouse architecture on top of data lakes. Delta Lake provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing on top of existing data lakes, such as S3, ADLS, GCS, and HDFS.\n",
    "\n",
    "Specifically, Delta Lake offers:\n",
    "\n",
    "- ACID transactions on Spark: Serializable isolation levels ensure that readers never see inconsistent data.\n",
    "\n",
    "- Scalable metadata handling: Leverages Spark distributed processing power to handle all the metadata for petabyte-scale tables with billions of files at ease.\n",
    "\n",
    "- Streaming and batch unification: A table in Delta Lake is a batch table as well as a streaming source and sink. Streaming data ingest, batch historic backfill, interactive queries all just work out of the box.\n",
    "\n",
    "- Schema enforcement: Automatically handles schema variations to prevent insertion of bad records during ingestion.\n",
    "\n",
    "- Time travel: Data versioning enables rollbacks, full historical audit trails, and reproducible machine learning experiments.\n",
    "\n",
    "- Upserts and deletes: Supports merge, update and delete operations to enable complex use cases like change-data-capture, slowly-changing-dimension (SCD) operations, streaming upserts, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb438a83-df2a-498a-aa94-5c3b5cee1eed",
   "metadata": {},
   "source": [
    "## What Is a Lakehouse?\n",
    "Few systems are beginning to emerge that address the limitations of data lakes. A lakehouse is a new, open architecture that combines the best elements of data lakes and data warehouses. Lakehouses are enabled by a new system design: implementing similar data structures and data management features to those in a data warehouse directly on top of low cost cloud storage in open formats. They are what you would get if you had to redesign data warehouses in the modern world, now that cheap and highly reliable storage (in the form of object stores) are available.\n",
    "\n",
    "A lakehouse has the following key features:\n",
    "\n",
    "- Transaction support: In an enterprise lakehouse many data pipelines will often be reading and writing data concurrently. Support for ACID transactions ensures consistency as multiple parties concurrently read or write data, typically using SQL.\n",
    "Schema enforcement and governance: The Lakehouse should have a way to support schema enforcement and evolution, supporting DW schema architectures such as star/snowflake-schemas. The system should be able to reason about data integrity, and it should have robust governance and auditing mechanisms.\n",
    "- BI support: Lakehouses enable using BI tools directly on the source data. This reduces staleness and improves recency, reduces latency, and lowers the cost of having to operationalize two copies of the data in both a data lake and a warehouse.\n",
    "Storage is decoupled from compute: In practice this means storage and compute use separate clusters, thus these systems are able to scale to many more concurrent users and larger data sizes. Some modern data warehouses also have this property.\n",
    "- Openness: The storage formats they use are open and standardized, such as Parquet, and they provide an API so a variety of tools and engines, including machine learning and Python/R libraries, can efficiently access the data directly.\n",
    "- Support for diverse data types ranging from unstructured to structured data: The lakehouse can be used to store, refine, analyze, and access data types needed for many new data applications, including images, video, audio, semi-structured data, and text.\n",
    "- Support for diverse workloads: including data science, machine learning, and SQL and analytics. Multiple tools might be needed to support all these workloads but they all rely on the same data repository.\n",
    "- End-to-end streaming: Real-time reports are the norm in many enterprises. Support for streaming eliminates the need for separate systems dedicated to serving real-time data applications.\n",
    "\n",
    "<center> <img src='img/warehouseVSlakeVslakehouse.png'> </center>\n",
    "\n",
    "Benefits of a lakehouse architecture:\n",
    "- Simple data model\n",
    "- Easy to understand and implement\n",
    "- Enables incremental ETL\n",
    "- Can recreate your tables from raw data at any time\n",
    "- ACID transactions, time travel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4bcd3f-7148-4a96-b1f8-12ac59cefc81",
   "metadata": {},
   "source": [
    "## DeltaLake Highlight Features\n",
    "- [Pandas to DeltaLake](https://delta.io/blog/2022-10-15-version-pandas-dataset/)\n",
    "- [Change Data feed (CDF)](https://docs.delta.io/latest/delta-change-data-feed.html)\n",
    "- [Table deletes, updates, and merges](https://docs.delta.io/latest/delta-update.html)\n",
    "- [Table utility commands](https://docs.delta.io/latest/delta-utility.html#history-schema):\n",
    "- [TimeTravel](https://docs.delta.io/latest/quick-start.html#-read-older-versions-of-data-using-time-travel)\r\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f5fbc-cb60-4ac2-9f3f-e02717bb6cb4",
   "metadata": {},
   "source": [
    "## Medallion Arquitecture (Multi Hopp):\n",
    "\n",
    "What is a medallion architecture?\n",
    "A medallion architecture is a data design pattern used to logically organize data in a lakehouse, with the goal of incrementally and progressively improving the structure and quality of data as it flows through each layer of the architecture (from Bronze ⇒ Silver ⇒ Gold layer tables). Medallion architectures are sometimes also referred to as \"multi-hop\" architectures.\n",
    "\n",
    "<center> <img src='img/multihop.png'> </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5af9479-a034-4dd9-9279-75102ebee48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from delta import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sql = lambda statement, limit=5: spark.sql(statement).limit(limit).toPandas()\n",
    "minutes_to_seconds = lambda x: x * 60\n",
    "run_every_n_seconds = minutes_to_seconds(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19f7b808-6b41-4593-8792-b531edc8687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "from glob import glob\n",
    "\n",
    "MAIN_PATH = \"speed_test/\"\n",
    "TEMP_PATH = f\"{MAIN_PATH}temp/\"\n",
    "RAW_PATH = f\"{MAIN_PATH}raw_data/\"\n",
    "CHECK_POINT = f\"{MAIN_PATH}_checkpoint\"\n",
    "os.makedirs(RAW_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "def clean_project():\n",
    "    try:\n",
    "        shutil.rmtree(\"spark-warehouse\")\n",
    "        shutil.rmtree(\"metastore_db\")\n",
    "        shutil.rmtree(\"speed_test/_checkpoint\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "clean_project()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efff52d7-569a-4d96-a80a-72d024f97780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speed_test():\n",
    "\n",
    "    def move_temp_to_raw():\n",
    "        files = glob(f\"{TEMP_PATH}*.json\")\n",
    "        list(map(lambda x: os.rename(x, x.replace(TEMP_PATH, RAW_PATH)), files))\n",
    "\n",
    "    while True:\n",
    "        now = dt.now()\n",
    "        file_name = f\"log_{now.strftime('%Y_%m_%d_%H_%M_%s')}.json\"\n",
    "        subprocess.run(\n",
    "            f\"touch {TEMP_PATH}{file_name} && speedtest --format=json >> {TEMP_PATH}{file_name} && mv  {TEMP_PATH}{file_name} {RAW_PATH}{file_name}\",\n",
    "            shell=True,\n",
    "            executable=\"/bin/bash\",\n",
    "        )\n",
    "        move_temp_to_raw()\n",
    "        list(\n",
    "            map(\n",
    "                os.remove,\n",
    "                filter(\n",
    "                    lambda x: os.path.getsize(x) == 0,\n",
    "                    glob(\"speed_test/raw_data/*.json\"),\n",
    "                ),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587ea9e0-b71b-4eb3-868f-5afec8ba4c85",
   "metadata": {},
   "source": [
    "<center><img src='img/example.png'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89776d19-cf65-42d7-9c26-19e104fc6e04",
   "metadata": {},
   "source": [
    "### Bronze layer (raw data)\r\n",
    "The Bronze layer is where we land all the data from external source systems. The table structures in this layer correspond to the source system table structures \"as-is,\" along with any additional metadata columns that capture the load date/time, process ID, etc. The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7437109-544a-4ee7-96df-d06ac7b55835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/17 10:35:30 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/17 10:35:30 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/17 10:35:32 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "24/04/17 10:35:32 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore hadoop@127.0.1.1\n",
      "24/04/17 10:35:32 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "24/04/17 10:35:33 WARN ObjectStore: Failed to get database speed_test, returning NoSuchObjectException\n",
      "24/04/17 10:35:33 WARN ObjectStore: Failed to get database speed_test, returning NoSuchObjectException\n",
      "24/04/17 10:35:33 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "24/04/17 10:35:33 WARN ObjectStore: Failed to get database speed_test, returning NoSuchObjectException\n",
      "24/04/17 10:35:35 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "24/04/17 10:35:37 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`speed_test`.`speed_test_logs` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/04/17 10:35:37 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/04/17 10:35:37 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/04/17 10:35:37 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/17 10:35:37 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/17 10:35:38 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`speed_test`.`silver_speed_test_logs` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/04/17 10:35:38 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`speed_test`.`summary_by_day_of_the_week` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    }
   ],
   "source": [
    "SCHEMA = \"speed_test\"\n",
    "\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {SCHEMA} \")\n",
    "\n",
    "spark.sql(\n",
    "    \"set spark.databricks.delta.changeDataFeed.timestampOutOfRange.enabled = true;\"\n",
    ")\n",
    "spark.sql(\"set SQLConf.ADAPTIVE_EXECUTION_ENABLED.key= true\")\n",
    "\n",
    "BRONZE_TABLE = f\"{SCHEMA}.speed_test_logs\"\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "  CREATE  TABLE IF NOT EXISTS {BRONZE_TABLE} \n",
    "  (`timestamp` Timestamp\n",
    "  ) USING DELTA\n",
    "  TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "SILVER_TABLE = f\"{SCHEMA}.silver_speed_test_logs\"\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "  CREATE  TABLE IF NOT EXISTS {SILVER_TABLE } \n",
    "  (`timestamp` Timestamp\n",
    "  ) USING DELTA\n",
    "  TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "GOLD_TABLE = f\"{SCHEMA}.summary_by_day_of_the_week\"\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "  CREATE  TABLE IF NOT EXISTS {GOLD_TABLE } \n",
    "  (`dayofweek` Integer\n",
    "  ) USING DELTA\n",
    "  TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d2c36a5-3ff3-4347-9726-48c2c9add3cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "      <th>operationMetrics</th>\n",
       "      <th>userMetadata</th>\n",
       "      <th>engineInfo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2024-04-17 10:35:34.031</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CREATE TABLE</td>\n",
       "      <td>{'description': None, 'partitionBy': '[]', 'pr...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Serializable</td>\n",
       "      <td>True</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>Apache-Spark/3.5.1 Delta-Lake/3.1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version               timestamp userId userName     operation  \\\n",
       "0        0 2024-04-17 10:35:34.031   None     None  CREATE TABLE   \n",
       "\n",
       "                                 operationParameters   job notebook clusterId  \\\n",
       "0  {'description': None, 'partitionBy': '[]', 'pr...  None     None      None   \n",
       "\n",
       "   readVersion isolationLevel  isBlindAppend operationMetrics userMetadata  \\\n",
       "0          NaN   Serializable           True               {}         None   \n",
       "\n",
       "                            engineInfo  \n",
       "0  Apache-Spark/3.5.1 Delta-Lake/3.1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql(f\"DESCRIBE HISTORY  {BRONZE_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bd13f1b-f12a-496a-b26e-db0895a47699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_bronze_table():\n",
    "\n",
    "    streaming_logs_schema = StructType(\n",
    "        [\n",
    "            StructField(\n",
    "                \"download\",\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"bandwidth\", LongType(), True),\n",
    "                        StructField(\"bytes\", LongType(), True),\n",
    "                        StructField(\"elapse\", LongType(), True),\n",
    "                        StructField(\"latency\", StringType(), True),\n",
    "                    ]\n",
    "                ),\n",
    "                True,\n",
    "            ),\n",
    "            StructField(\n",
    "                \"ping\",\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"jitter\", FloatType(), True),\n",
    "                        StructField(\"latency\", FloatType(), True),\n",
    "                        StructField(\"low\", FloatType(), True),\n",
    "                        StructField(\"high\", FloatType(), True),\n",
    "                    ]\n",
    "                ),\n",
    "                True,\n",
    "            ),\n",
    "            StructField(\"isp\", StringType(), True),\n",
    "            StructField(\n",
    "                \"result\",\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"id\", StringType(), True),\n",
    "                        StructField(\"url\", StringType(), True),\n",
    "                        StructField(\"persisted\", BooleanType(), True),\n",
    "                    ]\n",
    "                ),\n",
    "                True,\n",
    "            ),\n",
    "            StructField(\n",
    "                \"server\",\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"id\", StringType(), True),\n",
    "                        StructField(\"host\", StringType(), True),\n",
    "                        StructField(\"port\", LongType(), True),\n",
    "                        StructField(\"name\", StringType(), True),\n",
    "                        StructField(\"location\", StringType(), True),\n",
    "                        StructField(\"country\", StringType(), True),\n",
    "                        StructField(\"ip\", StringType(), True),\n",
    "                    ]\n",
    "                ),\n",
    "                True,\n",
    "            ),\n",
    "            StructField(\"timestamp\", TimestampType(), True),\n",
    "            StructField(\"type\", StringType(), True),\n",
    "            StructField(\n",
    "                \"upload\",\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"bandwidth\", LongType(), True),\n",
    "                        StructField(\"bytes\", LongType(), True),\n",
    "                        StructField(\"elapse\", LongType(), True),\n",
    "                        StructField(\"latency\", StringType(), True),\n",
    "                    ]\n",
    "                ),\n",
    "                True,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    streaming_logs_sdf = (\n",
    "        spark.readStream.option(\"ignoreCorruptFiles\", \"true\")\n",
    "        .schema(streaming_logs_schema)\n",
    "        .json(RAW_PATH)\n",
    "        .where(F.col(\"timestamp\").isNotNull())\n",
    "        .withColumn(\"log_file\", F.input_file_name())\n",
    "    )\n",
    "\n",
    "    streaming_logs_sdf.writeStream.format(\"delta\").outputMode(\"append\").option(\n",
    "        \"checkpointLocation\", CHECK_POINT\n",
    "    ).trigger(availableNow=True).option(\"overwriteSchema\", \"true\").option(\n",
    "        \"mergeSchema\", \"true\"\n",
    "    ).toTable(\n",
    "        BRONZE_TABLE\n",
    "    ).awaitTermination()\n",
    "    print(\"BRONZE UPDATED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29622ef-08fb-41f2-be17-a4d2a3dd3c9c",
   "metadata": {},
   "source": [
    "### Silver layer (cleansed and conformed data)\n",
    "In the Silver layer of the lakehouse, the data from the Bronze layer is matched, merged, conformed and cleansed (\"just-enough\") so that the Silver layer can provide an \"Enterprise view\" of all its key business entities, concepts and transactions. (e.g. master customers, stores, non-duplicated transactions and cross-reference tables).\n",
    "\n",
    "The Silver layer brings the data from different sources into an Enterprise view and enables self-service analytics for ad-hoc reporting, advanced analytics and ML. It serves as a source for Departmental Analysts, Data Engineers and Data Scientists to further create projects and analysis to answer business problems via enterprise and departmental data projects in the Gold Layer.\n",
    "\n",
    "In the lakehouse data engineering paradigm, typically the ELT methodology is followed vs. ETL - which means only minimal or \"just-enough\" transformations and data cleansing rules are applied while loading the Silver layer. Speed and agility to ingest and deliver the data in the data lake is prioritized, and a lot of project-specific complex transformations and business rules are applied while loading the data from the Silver to Gold layer. From a data modeling perspective, the Silver Layer has more 3rd-Normal Form like data models. Data Vault-like, write-performant data models can be used in this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67b5242a-b0ad-4db4-91e9-fd5974a2d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_silver_table():\n",
    "    bronze_delta_table = DeltaTable.forName(spark, BRONZE_TABLE)\n",
    "    silver_delta_table = DeltaTable.forName(spark, SILVER_TABLE)\n",
    "\n",
    "    def clean_bronze_table(sdf):\n",
    "        sdf = (\n",
    "            sdf.select(\n",
    "                [\n",
    "                    F.col(\"result\").getItem(\"id\").alias(\"test_id\"),\n",
    "                    F.col(\"timestamp\"),\n",
    "                    F.dayofweek(F.col(\"timestamp\")).alias(\"dayofweek\"),\n",
    "                    F.to_date(\"timestamp\").alias(\"date\"),\n",
    "                    F.hour(\"timestamp\").alias(\"hour\"),\n",
    "                    \"download\",\n",
    "                    \"upload\",\n",
    "                    \"isp\",\n",
    "                    F.col(\"server\").getItem(\"name\").alias(\"server_name\"),\n",
    "                    F.col(\"server\").getItem(\"ip\").alias(\"server_ip\"),\n",
    "                    F.col(\"server\").getItem(\"location\").alias(\"server_location\"),\n",
    "                    F.col(\"server\").getItem(\"country\").alias(\"server_country\"),\n",
    "                ]\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"part_of_the_day\",\n",
    "                (\n",
    "                    F.when((F.col(\"hour\") >= 5) & (F.col(\"hour\") < 12), \"Morning\")\n",
    "                    .when((F.col(\"hour\") >= 12) & (F.col(\"hour\") < 17), \"Afternoon\")\n",
    "                    .when((F.col(\"hour\") >= 17) & (F.col(\"hour\") < 21), \"Evening\")\n",
    "                    .otherwise(\"Night\")\n",
    "                ),\n",
    "            )\n",
    "            .withColumn(\"download_Mbytes\", F.col(\"download\").getItem(\"bytes\") / 1000000)\n",
    "            .withColumn(\"upload_Mbytes\", F.col(\"upload\").getItem(\"bytes\") / 1000000)\n",
    "            .drop(\"download\")\n",
    "            .drop(\"upload\")\n",
    "        )\n",
    "        return sdf\n",
    "\n",
    "    if silver_delta_table.toDF().limit(1).count() == 0:\n",
    "\n",
    "        sdf = clean_bronze_table(spark.read.format(\"delta\").table(BRONZE_TABLE))\n",
    "        (\n",
    "            sdf.write.format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .option(\"mergeSchema\", \"true\")\n",
    "            .saveAsTable(SILVER_TABLE)\n",
    "        )\n",
    "        print(\"SILVER FIRST LOAD\")\n",
    "\n",
    "    else:\n",
    "        bronze_last_update = (\n",
    "            bronze_delta_table.history()\n",
    "            .where('operation != \"CREATE TABLE\"')\n",
    "            .select(F.max(\"timestamp\").alias(\"bronze_last_update\"))\n",
    "            .collect()[0][0]\n",
    "        )\n",
    "        silver_last_update = (\n",
    "            silver_delta_table.history()\n",
    "            .where('operation != \"CREATE TABLE\"')\n",
    "            .select(F.max(\"timestamp\").alias(\"bronze_last_update\"))\n",
    "            .collect()[0][0]\n",
    "        )\n",
    "        if bronze_last_update > silver_last_update:\n",
    "            sdf = clean_bronze_table(\n",
    "                spark.read.format(\"delta\")\n",
    "                .option(\"readChangeFeed\", \"true\")\n",
    "                .option(\"startingTimestamp\", str(silver_last_update))\n",
    "                .table(BRONZE_TABLE)\n",
    "            )\n",
    "            silver_delta_table.alias(\"sink\").merge(\n",
    "                sdf.alias(\"source\"), \"source.test_id = sink.test_id\"\n",
    "            ).whenNotMatchedInsertAll().execute()\n",
    "            print(\"SILVER UPDATED\")\n",
    "        else:\n",
    "            print(\"No updates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c53271-918e-402d-8849-c0a74b2de3d1",
   "metadata": {},
   "source": [
    "### Gold layer (curated business-level tables)\n",
    "Data in the Gold layer of the lakehouse is typically organized in consumption-ready \"project-specific\" databases. The Gold layer is for reporting and uses more de-normalized and read-optimized data models with fewer joins. The final layer of data transformations and data quality rules are applied here. Final presentation layer of projects such as Customer Analytics, Product Quality Analytics, Inventory Analytics, Customer Segmentation, Product Recommendations, Marking/Sales Analytics etc. fit in this layer. We see a lot of Kimball style star schema-based data models or Inmon style Data marts fit in this Gold Layer of the lakehouse.\n",
    "\n",
    "So you can see that the data is curated as it moves through the different layers of a lakehouse. In some cases, we also see that lot of Data Marts and EDWs from the traditional RDBMS technology stack are ingested into the lakehouse, so that for the first time Enterprises can do \"pan-EDW\" advanced analytics and ML - which was just not possible or too cost prohibitive to do on a traditional stack. (e.g. IoT/Manufacturing data is tied with Sales and Marketing data for defect analysis or health care genomics, EMR/HL7 clinical data markets are tied with financial claims data to create a Healthcare Data Lake for timely and improved patient care analytics.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66d0d526-356c-4cca-868f-13cb6d947182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dayofweek_names():\n",
    "    index = [1, 2, 3, 4, 5, 6, 7]\n",
    "    esp = [\"Domingo\", \"Lunes\", \"Marte\", \"Miercoles\", \"Jueve\", \"Viernes\", \"Sabado\"]\n",
    "    eng = [\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"]\n",
    "\n",
    "    from pandas import DataFrame\n",
    "\n",
    "    from deltalake.writer import write_deltalake\n",
    "\n",
    "    write_deltalake(\n",
    "        \"spark-warehouse/speed_test.db/day_of_the_week\",\n",
    "        DataFrame(data={\"dayofweek\": index, \"esp\": esp, \"eng\": eng}),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "963d64f6-a227-43a3-a3f2-c3d696056785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_test_by(sdf, group_columns):\n",
    "\n",
    "    return (\n",
    "        sdf.groupby(group_columns)\n",
    "        .agg(\n",
    "            F.countDistinct(\"test_id\").alias(\"no_test\"),\n",
    "            F.sum(\"download_Mbytes\").alias(\"total_download_mbytes_recored\"),\n",
    "            F.sum(\"upload_Mbytes\").alias(\"total_upload_mbytes_recored\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"mean_download_Mbytes\",\n",
    "            F.col(\"total_download_mbytes_recored\") / F.col(\"no_test\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"mean_upload_Mbytes\",\n",
    "            F.col(\"total_upload_mbytes_recored\") / F.col(\"no_test\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def update_gold_table():\n",
    "    dayofweek_sdf = (\n",
    "        spark.read.format(\"delta\")\n",
    "        .load(\"spark-warehouse/speed_test.db/day_of_the_week\")\n",
    "        .withColumn(\"dayofweek\", F.col(\"dayofweek\").cast(IntegerType()))\n",
    "    )\n",
    "    gold_delta_table = DeltaTable.forName(spark, GOLD_TABLE)\n",
    "    if gold_delta_table.toDF().limit(1).count() > 0:\n",
    "        silver_delta_table = DeltaTable.forName(spark, SILVER_TABLE)\n",
    "        silver_last_update = (\n",
    "            silver_delta_table.history()\n",
    "            .where('operation != \"CREATE TABLE\"')\n",
    "            .select(F.max(\"timestamp\").alias(\"bronze_last_update\"))\n",
    "            .collect()[0][0]\n",
    "        )\n",
    "        gold_last_update = (\n",
    "            gold_delta_table.history()\n",
    "            .where('operation != \"CREATE TABLE\"')\n",
    "            .select(F.max(\"timestamp\").alias(\"gold_last_update\"))\n",
    "            .collect()[0][0]\n",
    "        )\n",
    "        if silver_last_update > gold_last_update:\n",
    "            sdf = summary_test_by(\n",
    "                spark.read.format(\"delta\")\n",
    "                .option(\"readChangeFeed\", \"true\")\n",
    "                .option(\"startingTimestamp\", str(gold_last_update))\n",
    "                .table(SILVER_TABLE),\n",
    "                \"dayofweek\",\n",
    "            ).join(dayofweek_sdf, on=\"dayofweek\", how=\"right\")\n",
    "            (\n",
    "                gold_delta_table.alias(\"sink\")\n",
    "                .merge(sdf.alias(\"source\"), \"source.dayofweek = sink.dayofweek\")\n",
    "                .whenNotMatchedInsertAll()\n",
    "                .whenMatchedUpdate(\n",
    "                    set={\n",
    "                        \"no_test\": \"source.no_test + sink.no_test\",\n",
    "                        \"total_download_mbytes_recored\": \"source.total_download_mbytes_recored + sink.total_download_mbytes_recored\",\n",
    "                        \"total_upload_mbytes_recored\": \"source.total_upload_mbytes_recored  + sink.total_upload_mbytes_recored\t\",\n",
    "                    }\n",
    "                )\n",
    "                .execute()\n",
    "            )\n",
    "            print(\"GOLD UPDATED\")\n",
    "        else:\n",
    "            print(\"GOLD No Updates\")\n",
    "\n",
    "    else:\n",
    "        silver_sdf = spark.read.format(\"delta\").table(SILVER_TABLE)\n",
    "        summary_test_by(silver_sdf, \"dayofweek\").join(\n",
    "            dayofweek_sdf, on=\"dayofweek\", how=\"right\"\n",
    "        ).write.format(\"delta\").mode(\"append\").option(\n",
    "            \"mergeSchema\", \"true\"\n",
    "        ).saveAsTable(\n",
    "            GOLD_TABLE\n",
    "        )\n",
    "        print(\"Gold Table first load\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de78a980-24c0-4792-a1d4-d2455e286d72",
   "metadata": {},
   "source": [
    "## Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcc62b07-e07d-4d10-b24a-0a10f09246f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/17 10:40:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRONZE UPDATED\n",
      "No updates\n",
      "Gold Table first load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/17 10:40:28 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/04/17 10:40:28 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/17 10:40:28 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/17 10:40:28 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/04/17 10:40:28 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/17 10:40:28 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "create_dayofweek_names()\n",
    "\n",
    "\n",
    "def run_pipeline():\n",
    "    #speed_test()\n",
    "    update_bronze_table()  # readStream\n",
    "    update_silver_table()  # ChangeDataFeed and append last updates\n",
    "    update_gold_table()  # ChangeDataFeed , Update and insert new Data\n",
    "    \n",
    "\n",
    "run_pipeline();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "555c2f30-7ab3-418b-83c7-bb1ebafc49cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:8050/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/17 10:40:42 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/04/17 10:40:42 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/17 10:40:42 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/17 10:40:42 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/04/17 10:40:42 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/17 10:40:42 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/17 10:40:52 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/04/17 10:40:52 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/17 10:40:52 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/17 10:40:52 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/04/17 10:40:52 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/17 10:40:52 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/17 10:41:02 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/04/17 10:41:02 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/17 10:41:02 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/17 10:41:02 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/04/17 10:41:02 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/17 10:41:02 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/17 10:41:12 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/04/17 10:41:12 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/17 10:41:12 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/17 10:41:12 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/04/17 10:41:12 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/17 10:41:12 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/17 10:41:22 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/04/17 10:41:22 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/17 10:41:22 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/17 10:41:22 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/04/17 10:41:22 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/17 10:41:22 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "plot_bgcolor = \"#2c292d\"\n",
    "# paper_bgcolor =\"#211f22\"\n",
    "paper_bgcolor = \"#1a1d21\"\n",
    "download_color = \"#ab9df2\"\n",
    "upload_color = \"#78dce8\"\n",
    "default_fontcolor = \"white\"\n",
    "\n",
    "import dash\n",
    "import dash_bootstrap_components as dbc\n",
    "import plotly.graph_objects as go\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "from pandas import melt\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "def line_chart_download_vs_upload(fig, df):\n",
    "\n",
    "    x = df.timestamp\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x, y=df.upload_Mbytes, name=\"Upload (Mbps)\", line=dict(color=upload_color)\n",
    "        ),\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"<b>Upload (Mbps)</b>\",\n",
    "        color=upload_color,\n",
    "        rangemode=\"tozero\",\n",
    "        showgrid=False,\n",
    "        row=1,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=x,\n",
    "            y=df.download_Mbytes,\n",
    "            name=\"Download (Mbps)\",\n",
    "            line=dict(color=download_color),\n",
    "        ),\n",
    "        row=2,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"<b>Download (Mbps)</b>\",\n",
    "        color=download_color,\n",
    "        rangemode=\"tozero\",\n",
    "        showgrid=False,\n",
    "        row=2,\n",
    "        col=2,\n",
    "    )\n",
    "\n",
    "    fig.update_xaxes(\n",
    "        showgrid=False,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor=plot_bgcolor,\n",
    "        paper_bgcolor=paper_bgcolor,\n",
    "        font=dict(color=\"white\"),\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.15, xanchor=\"right\", x=1),\n",
    "    )\n",
    "\n",
    "\n",
    "def gauges_indicators(fig, value):\n",
    "\n",
    "    def gauge_chart(value, steps, title, color):\n",
    "        max_step = steps[-1][-1]\n",
    "        title = f\"{title} <span style='font-size:0.8em;color:gray'>MBps</span><br><span style='font-size:0.5em;color:gray'>Average</span>\"\n",
    "        gauge = go.Indicator(\n",
    "            mode=\"gauge+number+delta\",\n",
    "            value=value,\n",
    "            domain={\"x\": [0.25, 0.55], \"y\": [0.25, 0.55]},\n",
    "            title={\"text\": title, \"font\": {\"size\": 25}, \"align\": \"center\"},\n",
    "            delta={\n",
    "                \"reference\": steps[-1][0],\n",
    "                \"font\": {\"size\": 13},\n",
    "                \"increasing\": {\"color\": color},\n",
    "            },\n",
    "            number={\"font\": {\"size\": 25}},\n",
    "            gauge={\n",
    "                \"axis\": {\n",
    "                    \"range\": [None, max_step],\n",
    "                    \"tickwidth\": 2,\n",
    "                    \"tickcolor\": plot_bgcolor,\n",
    "                },\n",
    "                \"bar\": {\"color\": color},\n",
    "                \"bgcolor\": \"white\",\n",
    "                \"borderwidth\": 2,\n",
    "                \"bordercolor\": \"gray\",\n",
    "                \"steps\": [\n",
    "                    {\"range\": steps[0], \"color\": \"#ff6188\"},\n",
    "                    {\"range\": steps[1], \"color\": \"#fc9867\"},\n",
    "                    {\"range\": steps[2], \"color\": \"#a9dc76\"},\n",
    "                ],\n",
    "            },\n",
    "        )\n",
    "\n",
    "        return gauge\n",
    "\n",
    "    fig.add_trace(\n",
    "        gauge_chart(\n",
    "            value[\"upload_Mbytes\"],\n",
    "            steps=[[0, 10], [10, 15], [15, 20]],\n",
    "            title=f\"<span style='font-size:0.8em;color:{upload_color}'>Upload</span>\",\n",
    "            color=upload_color,\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.add_trace(\n",
    "        gauge_chart(\n",
    "            value[\"download_Mbytes\"],\n",
    "            steps=[[0, 450], [450, 600], [600, 700]],\n",
    "            title=f\"<span style='font-size:0.8em;color:{download_color}'>Download</span>\",\n",
    "            color=download_color,\n",
    "        ),\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        paper_bgcolor=paper_bgcolor,\n",
    "        font={\"color\": \"white\", \"family\": \"Arial\"},\n",
    "        showlegend=False,\n",
    "    )\n",
    "    fig.update_traces(number=dict(font=dict(size=28)), delta=dict(font=dict(size=25)))\n",
    "\n",
    "\n",
    "def Heatmaps():\n",
    "    df = (\n",
    "        spark.read.format(\"delta\")\n",
    "        .table(GOLD_TABLE)\n",
    "        .withColumn(\n",
    "            \"mean_download_Mbytes\",\n",
    "            F.col(\"total_download_mbytes_recored\") / F.col(\"no_test\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"mean_upload_Mbytes\",\n",
    "            F.col(\"total_upload_mbytes_recored\") / F.col(\"no_test\")\n",
    "        )\n",
    "        .toPandas()\n",
    "        .set_index(\"dayofweek\")\n",
    "        .assign(\n",
    "            download=lambda x: x[\"mean_download_Mbytes\"].apply(\n",
    "                lambda _x: 600 if _x > 600 else _x\n",
    "            )\n",
    "            / 600\n",
    "        )\n",
    "        .assign(\n",
    "            upload=lambda x: x[\"mean_upload_Mbytes\"].apply(\n",
    "                lambda _x: 15 if _x > 15 else _x\n",
    "            )\n",
    "            / 15\n",
    "        )[[\"download\", \"upload\", \"eng\"]]\n",
    "    ).sort_index()\n",
    "    df = melt(df, id_vars=[\"eng\"], ignore_index=False).fillna(0)\n",
    "    fig = go.Figure(\n",
    "        data=go.Heatmap(\n",
    "            x=df.eng,\n",
    "            z=df[\"value\"],\n",
    "            y=df[\"variable\"],\n",
    "            colorscale=\"Spectral\",\n",
    "            zmax=1,\n",
    "            zmin=0,\n",
    "        )\n",
    "    )\n",
    "    fig.layout.update(\n",
    "        paper_bgcolor=paper_bgcolor,\n",
    "        font={\"color\": \"white\", \"family\": \"Arial\"},\n",
    "        height=300,\n",
    "        margin=dict(l=0, r=0, b=20, t=10),\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "def multiplot_speedtest(df):\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        specs=[[{\"type\": \"domain\"}, {}], [{\"type\": \"domain\"}, {}]],\n",
    "        column_widths=[0.30, 0.70],\n",
    "        row_heights=[0.25, 0.25],\n",
    "        horizontal_spacing=0.15,\n",
    "        vertical_spacing=0.15,\n",
    "    )\n",
    "\n",
    "    values = df[[\"download_Mbytes\", \"upload_Mbytes\"]].iloc[-3:].mean()\n",
    "    gauges_indicators(fig, values)\n",
    "    line_chart_download_vs_upload(fig, df)\n",
    "    fig.update_layout(height=550, margin=dict(l=35, r=35, b=30, t=55))\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def register_Callback(app):\n",
    "    @app.callback(\n",
    "        Output(\"stream_line_chart\", \"figure\"),\n",
    "        [\n",
    "            Input(\"interval-component\", \"n_intervals\"),\n",
    "        ],\n",
    "    )\n",
    "    def streamFig(intervals):\n",
    "        df = (\n",
    "            spark.read.table(SILVER_TABLE)\n",
    "            .where(\n",
    "                F.col(\"timestamp\")\n",
    "                >= (F.current_timestamp() - F.expr(\"INTERVAL 60 minutes\"))\n",
    "            )\n",
    "            .orderBy(\"timestamp\", ascending=False)\n",
    "            .limit(10)\n",
    "            .toPandas()\n",
    "            .sort_values(\"timestamp\", ascending=True)\n",
    "        )\n",
    "        return multiplot_speedtest(df)\n",
    "\n",
    "    @app.callback(\n",
    "        Output(\"heatmaps\", \"figure\"),\n",
    "        [\n",
    "            Input(\"interval-component\", \"n_intervals\"),\n",
    "        ],\n",
    "    )\n",
    "    def heatMaps(intervals):\n",
    "        return Heatmaps()\n",
    "\n",
    "\n",
    "config = {\"displaylogo\": False, \"scrollZoom\": False, \"displayModeBar\": False}\n",
    "\n",
    "updates = dcc.Interval(\n",
    "    id=\"interval-component\", interval=10000, n_intervals=0  # in milliseconds\n",
    ")\n",
    "\n",
    "\n",
    "navbar = dbc.Navbar(\n",
    "    dbc.Container(\n",
    "        [\n",
    "            html.A(\n",
    "                # Use row and col to control vertical alignment of logo / brand\n",
    "                dbc.Row(\n",
    "                    [\n",
    "                        dbc.Col(\n",
    "                            html.Img(\n",
    "                                src=\"https://www.pinclipart.com/picdir/big/491-4917274_panama-flag-png-palestine-flag-vector-clipart.png\",\n",
    "                                height=\"30px\",\n",
    "                            )\n",
    "                        ),\n",
    "                        dbc.Col(\n",
    "                            dbc.NavbarBrand(\n",
    "                                \"Network Speed Test by Jose Quesada\", className=\"ms-2\"\n",
    "                            )\n",
    "                        ),\n",
    "                    ],\n",
    "                    align=\"center\",\n",
    "                    className=\"g-0\",\n",
    "                ),\n",
    "                href=\"https://plotly.com\",\n",
    "                style={\"textDecoration\": \"none\"},\n",
    "            ),\n",
    "            dbc.NavbarToggler(id=\"navbar-toggler\", n_clicks=0),\n",
    "        ]\n",
    "    ),\n",
    "    color=paper_bgcolor,\n",
    "    dark=True,\n",
    ")\n",
    "\n",
    "\n",
    "streaming_col = dbc.Col(dcc.Graph(id=\"stream_line_chart\", config=config))\n",
    "heatmap_col = dbc.Col(dcc.Graph(id=\"heatmaps\"))\n",
    "\n",
    "layout = dbc.Container(\n",
    "    [\n",
    "        navbar,\n",
    "        dbc.Container(\n",
    "            [\n",
    "                updates,\n",
    "                dcc.Store(id=\"last_32hrs\"),\n",
    "                dbc.Row(streaming_col),\n",
    "                dbc.Row(heatmap_col),\n",
    "            ],\n",
    "            style={\"background-color\": paper_bgcolor, \"color\": default_fontcolor},\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "app = dash.Dash(\n",
    "    external_stylesheets=[\n",
    "        \"https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css\"\n",
    "    ],\n",
    ")\n",
    "# app.config.suppress_callback_exceptions = True\n",
    "app.layout = layout\n",
    "register_Callback(app)\n",
    "app.run(jupyter_mode=\"external\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c652669f-35e8-46e4-bed5-c75b45308629",
   "metadata": {},
   "source": [
    "<img src='img/dashboard.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcca27b-729f-4c43-8e6a-a1c812cd7e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    run_pipeline()\n",
    "    time.sleep(run_every_n_seconds);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1197bfa-27ef-4cb1-8ddf-a269d89df950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(1);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
