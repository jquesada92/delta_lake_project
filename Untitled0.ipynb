{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jquesada92/delta_lake_project/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sWAKPzQzj1G",
        "outputId": "bb1760c1-bd19-47d7-d9ec-1fb73adc1d6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting delta-spark==3.0.0\n",
            "  Downloading delta_spark-3.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: pyspark<3.6.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from delta-spark==3.0.0) (3.5.1)\n",
            "Requirement already satisfied: importlib-metadata>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from delta-spark==3.0.0) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=1.0.0->delta-spark==3.0.0) (3.23.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark<3.6.0,>=3.5.0->delta-spark==3.0.0) (0.10.9.7)\n",
            "Downloading delta_spark-3.0.0-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: delta-spark\n",
            "Successfully installed delta-spark-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install delta-spark==3.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_PATH = '/content/drive/MyDrive/Colab Notebooks/contraloria'\n",
        "STAGING_PATH = f'{SOURCE_PATH}/staging'\n",
        "CHECKPOINT_PATH = f'{SOURCE_PATH}/checkpoint'\n",
        "spark_warehouse = f'{SOURCE_PATH}/spark-warehouse'\n",
        "bronze_path = f'{spark_warehouse}/bronze_scd_type_2'\n",
        "key_cols = ['cedula','institucion']\n",
        "update_col = 'fecha_actualizacion'"
      ],
      "metadata": {
        "id": "WQk8xy7_KEez"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1HvJEhOGzpgK"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from delta import *\n",
        "from delta.tables import DeltaTable\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "# Create SparkSession with Delta Lake configurations\n",
        "# Add the Delta Lake packages to the SparkSession configuration\n",
        "builder = SparkSession.builder \\\n",
        "    .appName(\"DeltaLakeAlternativeSession\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\",spark_warehouse )\\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_table_if_not_exists(df:DataFrame,table_path:str)->None:\n",
        "  if not spark.catalog.tableExists(table_path):\n",
        "    # Create the table if it does not exist\n",
        "    df.write.format(\"delta\").mode(\"overwrite\").save(table_path)\n",
        "    return False\n",
        "  else:\n",
        "    return True"
      ],
      "metadata": {
        "id": "QvVIkt3Z4bZL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c4s9qJAT6NPf"
      },
      "outputs": [],
      "source": [
        "\n",
        "def bronze_type2_upsert( microbatch_df: DataFrame, batch_id: str, table_path:str = bronze_path )-> None:\n",
        "    \"\"\"\n",
        "    Performs a Type 2 SCD upsert into the bronze Delta table.\n",
        "    - Deduplicates incoming microbatch by key, variationNumber, and timestamp.\n",
        "    - Adds SCD2 columns: start_date, end_date, is_current.\n",
        "    - If the table does not exist, creates it.\n",
        "    - Otherwise, merges to expire old records and appends new ones.\n",
        "    \"\"\"\n",
        "    if microbatch_df.isEmpty():\n",
        "        return\n",
        "\n",
        "    __window = lambda x: Window.partitionBy(*x).orderBy(F.desc(update_col))\n",
        "\n",
        "    # Deduplicate and add SCD2 columns\n",
        "    df_updates = (\n",
        "        microbatch_df.withColumn(\n",
        "            \"duplicated\",\n",
        "            F.row_number().over(__window(key_cols + [update_col])),\n",
        "        )\n",
        "        .where(\"duplicated = 1\")\n",
        "        .drop(\"duplicated\")\n",
        "        .withColumn(\n",
        "            \"row_num\",\n",
        "            F.row_number().over(__window(key_cols)),\n",
        "        )\n",
        "        .withColumn(\"start_date\", F.col(update_col))\n",
        "        .withColumn(\"end_date\", F.lag(update_col).over(__window(key_cols)))\n",
        "        .withColumn(\n",
        "            \"last_update\",\n",
        "            F.when(F.col(\"row_num\") == F.lit(1), F.lit(True)).otherwise(F.lit(False)),\n",
        "        )\n",
        "        .drop(\"row_num\")\n",
        "    )\n",
        "\n",
        "    create_table_if_not_exists(df_updates,table_path)\n",
        "\n",
        "    # Reference to the Delta table\n",
        "    delta_target = DeltaTable.forPath(spark, table_path)\n",
        "    updates = df_updates.alias(\"updates\")\n",
        "    target = delta_target.alias(\"target\")\n",
        "\n",
        "    # Merge condition on key columns\n",
        "    merge_condition = \" AND \".join([f\"target.{k} = updates.{k}\" for k in key_cols])\n",
        "\n",
        "    # 1. Mark old record as not current if a new version arrives\n",
        "    delta_target.alias(\"target\").merge(\n",
        "        updates.alias(\"updates\"),\n",
        "        f\"({merge_condition} AND target.last_update = true) AND (updates.last_update = true) AND (updates.{update_col} > target.{update_col})\",\n",
        "    ).whenMatchedUpdate(\n",
        "        set={\"end_date\": \"updates.start_date\", \"last_update\": F.lit(False)}\n",
        "    ).execute()\n",
        "\n",
        "    # 2. Always insert the new version as a new record\n",
        "    df_updates.write.format(\"delta\").mode(\"append\").save(table_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oc3-OdDn2Gq0"
      },
      "outputs": [],
      "source": [
        "schema =StructType([StructField('nombre', StringType(), True),\n",
        "                    StructField('apellido', StringType(), True),\n",
        "                    StructField('cedula', StringType(), True),\n",
        "                    StructField('cargo', StringType(), True),\n",
        "                    StructField('salario', DoubleType(), True),\n",
        "                    StructField('gasto', DoubleType(), True),\n",
        "                    StructField('estado', StringType(), True),\n",
        "                    StructField('fecha_de_inicio', DateType(), True),\n",
        "                    StructField('fecha_actualizacion', TimestampType(), True),\n",
        "                    StructField('fecha_consulta', TimestampType(), True),\n",
        "                    StructField('archivo', StringType(), True),\n",
        "                    StructField('institucion', StringType(), True)])\n",
        "\n",
        "\n",
        "source_staging_sdf =  (spark.readStream.format(\"parquet\")\n",
        "    .schema(schema)\n",
        "    .parquet(STAGING_PATH)\n",
        "    .withColumn('file_path', F.input_file_name()\n",
        ")\n",
        ")\n",
        "\n",
        "\n",
        "bronze_query = (\n",
        "    source_staging_sdf.writeStream\n",
        "    .trigger(availableNow=True)\n",
        "    .foreachBatch(lambda df, batch_id: bronze_type2_upsert( df, batch_id))\n",
        "    .option(\"checkpointLocation\",CHECKPOINT_PATH + '/bronze')\n",
        "    .outputMode(\"append\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "bronze_query.awaitTermination()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  spark.sql(f\"select * from delta.`{bronze_path}`\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq4ZIO1ld6ae",
        "outputId": "42dc086e-e743-4f0b-a092-2f7cf5eee678"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------+-------------+--------------------+-------+-----+----------+---------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-----------+\n",
            "|   nombre| apellido|       cedula|               cargo|salario|gasto|    estado|fecha_de_inicio|fecha_actualizacion|      fecha_consulta|             archivo|         institucion|           file_path|         start_date|           end_date|last_update|\n",
            "+---------+---------+-------------+--------------------+-------+-----+----------+---------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-----------+\n",
            "|     RUTH|  DEL CID|1-0016-000226|EDUCADOR B 1 (MAE...|2068.97|  0.0|PERMANENTE|     1993-03-22|2025-07-16 08:20:06|2025-07-19 15:55:...|InformeConsultaPl...|MINISTERIO DE EDU...|file:///content/d...|2025-07-16 08:20:06|               NULL|       true|\n",
            "|     RUTH|  DEL CID|1-0016-000226|EDUCADOR B 1 (MAE...|2068.97|  0.0|PERMANENTE|     1993-03-22|2025-02-03 08:20:06|2025-02-06 17:51:...|InformeConsultaPl...|MINISTERIO DE EDU...|file:///content/d...|2025-02-03 08:20:06|2025-07-16 08:20:06|      false|\n",
            "|     RUTH|  DEL CID|1-0016-000226|EDUCADOR B 1 (MAE...|2068.97|  0.0|PERMANENTE|     1993-03-22|2024-05-03 08:20:07|2024-05-13 20:53:...|InformeConsultaPl...|MINISTERIO DE EDU...|file:///content/d...|2024-05-03 08:20:07|2025-02-03 08:20:06|      false|\n",
            "|     RUTH|  DEL CID|1-0016-000226|EDUCADOR B 1 (MAE...|2068.97|  0.0|PERMANENTE|     1993-03-22|2024-04-02 08:20:09| 2024-04-09 19:38:40|InformeConsultaPl...|MINISTERIO DE EDU...|file:///content/d...|2024-04-02 08:20:09|2024-05-03 08:20:07|      false|\n",
            "|  AUGUSTO|    LOPEZ|1-0018-001966|EDUCADOR N 22 (PR...|2449.57|  0.0|PERMANENTE|     1987-04-07|2025-07-16 08:20:06|2025-07-19 15:55:...|InformeConsultaPl...|MINISTERIO DE EDU...|file:///content/d...|2025-07-16 08:20:06|               NULL|       true|\n",
            "|  AUGUSTO|    LOPEZ|1-0018-001966|EDUCADOR N 22 (PR...|2449.57|  0.0|PERMANENTE|     1987-04-07|2025-02-03 08:20:06|2025-02-06 17:51:...|InformeConsultaPl...|MINISTERIO DE EDU...|file:///content/d...|2025-02-03 08:20:06|2025-07-16 08:20:06|      false|\n",
            "|  AUGUSTO|    LOPEZ|1-0018-001966|EDUCADOR N 22 (PR...|2449.57|  0.0|PERMANENTE|     1987-04-07|2024-05-03 08:20:07|2024-05-13 20:53:...|InformeConsultaPl...|MINISTERIO DE EDU...|file:///content/d...|2024-05-03 08:20:07|2025-02-03 08:20:06|      false|\n",
            "|  AUGUSTO|    LOPEZ|1-0018-001966|EDUCADOR N 22 (PR...|2449.57|  0.0|PERMANENTE|     1987-07-04|2024-04-02 08:20:09| 2024-04-09 19:38:40|InformeConsultaPl...|MINISTERIO DE EDU...|file:///content/d...|2024-04-02 08:20:09|2024-05-03 08:20:07|      false|\n",
            "|   ISABEL|  MORALES|1-0018-002662|       ESCRIBIENTE I|1194.13|  0.0|PERMANENTE|     2009-03-12|2025-07-16 08:20:06|2025-07-19 15:58:...|InformeConsultaPl...|     ORGANO JUDICIAL|file:///content/d...|2025-07-16 08:20:06|               NULL|       true|\n",
            "|   ISABEL|  MORALES|1-0018-002662|       ESCRIBIENTE I|1194.13|  0.0|PERMANENTE|     2009-03-12|2025-02-03 08:20:06|2025-02-06 17:53:...|InformeConsultaPl...|     ORGANO JUDICIAL|file:///content/d...|2025-02-03 08:20:06|2025-07-16 08:20:06|      false|\n",
            "|   ISABEL|  MORALES|1-0018-002662|       ESCRIBIENTE I|1194.13|  0.0|PERMANENTE|     2009-03-12|2024-05-03 08:20:07|2024-05-13 20:57:...|InformeConsultaPl...|     ORGANO JUDICIAL|file:///content/d...|2024-05-03 08:20:07|2025-02-03 08:20:06|      false|\n",
            "|   ISABEL|  MORALES|1-0018-002662|       ESCRIBIENTE I|1194.13|  0.0|PERMANENTE|     2009-12-03|2024-04-02 08:20:09| 2024-04-09 19:38:40|InformeConsultaPl...|     ORGANO JUDICIAL|file:///content/d...|2024-04-02 08:20:09|2024-05-03 08:20:07|      false|\n",
            "|   ISABEL|  MORALES|1-0018-002662|       ESCRIBIENTE I| 944.13|  0.0|PERMANENTE|     2009-12-03|2024-02-19 08:20:07| 2024-02-29 20:55:34|InformeConsultaPl...|     ORGANO JUDICIAL|file:///content/d...|2024-02-19 08:20:07|2024-04-02 08:20:09|      false|\n",
            "|ESMERALDA|    GOUGH|1-0020-000431|EDUCADOR I 4 (PRO...| 1776.1|  0.0|PERMANENTE|     2016-02-23|2025-07-16 08:20:06|2025-07-19 15:55:...|InformeConsultaPl...|MINISTERIO DE EDU...|file:///content/d...|2025-07-16 08:20:06|               NULL|       true|\n",
            "|ESMERALDA|    GOUGH|1-0020-000431|EDUCADOR I 4 (PRO...| 1776.1|  0.0|PERMANENTE|     2016-02-23|2025-02-03 08:20:06|2025-02-06 17:51:...|InformeConsultaPl...|MINISTERIO DE EDU...|file:///content/d...|2025-02-03 08:20:06|2025-07-16 08:20:06|      false|\n",
            "|ESMERALDA|    GOUGH|1-0020-000431|EDUCADOR I 4 (PRO...| 1776.1|  0.0|PERMANENTE|     2016-02-23|2024-05-03 08:20:07|2024-05-13 20:53:...|InformeConsultaPl...|MINISTERIO DE EDU...|file:///content/d...|2024-05-03 08:20:07|2025-02-03 08:20:06|      false|\n",
            "|ESMERALDA|    GOUGH|1-0020-000431|EDUCADOR I 4 (PRO...| 1776.1|  0.0|PERMANENTE|     2016-02-23|2024-04-02 08:20:09| 2024-04-09 19:38:40|InformeConsultaPl...|MINISTERIO DE EDU...|file:///content/d...|2024-04-02 08:20:09|2024-05-03 08:20:07|      false|\n",
            "| DIANELSA|RODRIGUEZ|1-0022-001935|EDUCADOR F 1 (MAE...|1934.18|  0.0|PERMANENTE|     2005-03-07|2025-07-16 08:20:06|2025-07-19 15:55:...|InformeConsultaPl...|MINISTERIO DE EDU...|file:///content/d...|2025-07-16 08:20:06|               NULL|       true|\n",
            "| DIANELSA|RODRIGUEZ|1-0022-001935|EDUCADOR F 1 (MAE...|1934.18|  0.0|PERMANENTE|     2005-03-07|2025-02-03 08:20:06|2025-02-06 17:51:...|InformeConsultaPl...|MINISTERIO DE EDU...|file:///content/d...|2025-02-03 08:20:06|2025-07-16 08:20:06|      false|\n",
            "| DIANELSA|RODRIGUEZ|1-0022-001935|EDUCADOR F 1 (MAE...|1934.18|  0.0|PERMANENTE|     2005-03-07|2024-05-03 08:20:07|2024-05-13 20:53:...|InformeConsultaPl...|MINISTERIO DE EDU...|file:///content/d...|2024-05-03 08:20:07|2025-02-03 08:20:06|      false|\n",
            "+---------+---------+-------------+--------------------+-------+-----+----------+---------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bronze_sdf = spark.read.format('delta').load(bronze_path)\\\n",
        "                    .where(\"last_update = true\")\\\n",
        "                    .withColumn('activate',F.lit(True))\n",
        "\n",
        "create_table_if_not_exists(bronze_sdf,'silver_scd_type_2')\n",
        "\n",
        "updates_sdf = bronze_sdf.withColumn('last_update', F.col('fecha'))\n"
      ],
      "metadata": {
        "id": "PsOunebn4GBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def silver_upsert( microbatch_df: DataFrame, batch_id: str, table_name:str = bronze_table )-> None:\n",
        "    \"\"\"\n",
        "    Performs a Type 2 SCD upsert into the bronze Delta table.\n",
        "    - Deduplicates incoming microbatch by key, variationNumber, and timestamp.\n",
        "    - Adds SCD2 columns: start_date, end_date, is_current.\n",
        "    - If the table does not exist, creates it.\n",
        "    - Otherwise, merges to expire old records and appends new ones.\n",
        "    \"\"\"\n",
        "    if microbatch_df.isEmpty():\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "    __window = lambda x: Window.partitionBy(*x).orderBy(F.desc(update_col))\n",
        "\n",
        "    # Deduplicate and add SCD2 columns\n",
        "    df_updates = (\n",
        "        microbatch_df.withColumn(\n",
        "            \"duplicated\",\n",
        "            F.row_number().over(__window(key_cols + [update_col])),\n",
        "        )\n",
        "        .where(\"duplicated = 1\")\n",
        "        .drop(\"duplicated\")\n",
        "        .withColumn(\n",
        "            \"row_num\",\n",
        "            F.row_number().over(__window(key_cols)),\n",
        "        )\n",
        "        .withColumn(\"start_date\", F.col(update_col))\n",
        "        .withColumn(\"end_date\", F.lag(update_col).over(__window(key_cols)))\n",
        "        .withColumn(\n",
        "            \"last_update\",\n",
        "            F.when(F.col(\"row_num\") == F.lit(1), F.lit(True)).otherwise(F.lit(False)),\n",
        "        )\n",
        "        .drop(\"row_num\")\n",
        "    )\n",
        "\n",
        "    create_table_if_not_exists(df_updates,table_name)\n",
        "\n",
        "    # Reference to the Delta table\n",
        "    delta_target = DeltaTable.forName(spark, table_name)\n",
        "    updates = df_updates.alias(\"updates\")\n",
        "    target = delta_target.alias(\"target\")\n",
        "\n",
        "    # Merge condition on key columns\n",
        "    merge_condition = \" AND \".join([f\"target.{k} = updates.{k}\" for k in key_cols])\n",
        "\n",
        "    # 1. Mark old record as not current if a new version arrives\n",
        "    delta_target.alias(\"target\").merge(\n",
        "        updates.alias(\"updates\"),\n",
        "        f\"({merge_condition} AND target.last_update = true) AND (updates.last_update = true) AND (updates.{update_col} > target.{update_col})\",\n",
        "    ).whenMatchedUpdate(\n",
        "        set={\"end_date\": \"updates.start_date\", \"last_update\": F.lit(False)}\n",
        "    ).execute()\n",
        "\n",
        "    # 2. Always insert the new version as a new record\n",
        "    df_updates.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n",
        "\n"
      ],
      "metadata": {
        "id": "S9jeAt-pCAjN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1w9OA6St7-3qQEZmVL-qULLrL_sMBBuMC",
      "authorship_tag": "ABX9TyPXmBGnGA6OtF4XAxuqaQgf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}