{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc27ae7-4f3a-405b-a054-4aa34e0a737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from delta import *\n",
    "from config import *\n",
    "import os\n",
    "\n",
    "\n",
    "spark.sql(\"set spark.sql.streaming.schemaInference=true\")\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "spark.sql(\"set spark.databricks.delta.changeDataFeed.timestampOutOfRange.enabled = true;\")\n",
    "\n",
    "os.makedirs(bronze_location,exist_ok = True)\n",
    "os.makedirs(silver_location,exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aac36e6a-893f-448b-bae0-6691114ff413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/15 20:19:14 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/02/15 20:19:14 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/02/15 20:19:15 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "24/02/15 20:19:15 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore hadoop@127.0.1.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema}\")\n",
    "\n",
    "spark.sql(\n",
    "    f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS  {schema}.{bronze_table} (\n",
    "    nombre string, \n",
    "    apellido string, \n",
    "    cedula string, \n",
    "    salario double, \n",
    "    gasto double, \n",
    "    estado string, \n",
    "    fecha_de_inicio date, \n",
    "    fecha_actualizacion timestamp, \n",
    "    fecha_consulta timestamp, \n",
    "    institucion string,\n",
    "    nombre_reporte string\n",
    "    )\n",
    "USING delta\n",
    "LOCATION '{bronze_location}'\n",
    "TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cdedb2b-3a13-4b47-b082-faef6a2a9621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/15 20:18:00 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "24/02/15 20:18:03 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def load_raw_into_bronze():\n",
    "    \n",
    "    raw_data_sdf =(spark.readStream.schema(StructType([\n",
    "                 StructField(\"Nombre\", StringType(), True),\n",
    "                 StructField(\"Apellido\", StringType(), True),\n",
    "                 StructField(\"Cédula\",StringType(), True),\n",
    "                 StructField(\"Salario\", DoubleType(), True),\n",
    "                 StructField(\"Gasto\", DoubleType(), True),\n",
    "                 StructField(\"Estado\", StringType(), True),\n",
    "                 StructField(\"Fecha de inicio\", StringType(), True),\n",
    "                StructField(\"Fecha Actualizacion\", TimestampType(), True),\n",
    "                 StructField(\"Fecha Consulta\", TimestampType(), True),\n",
    "                 StructField(\"Institucion\",StringType(), True)]))\n",
    "                .parquet(\"s3a://pty-planilla-publica/\")\n",
    "                  )\n",
    "    \n",
    "    columns = raw_data_sdf.columns\n",
    "    \n",
    "    # Load Raw Data into Bronze\n",
    "    \n",
    "    \n",
    "    \n",
    "    (\n",
    "    raw_data_sdf\n",
    "    .select(list(map(lambda x: F.col(x).alias(x.lower().replace(' ','_')),columns)))\n",
    "    .withColumnRenamed('cédula','cedula')\n",
    "    .withColumn('fecha_de_inicio',F.to_date(F.col('fecha_de_inicio'),'dd/MM/yyyy'))\n",
    "    .withColumn('nombre_reporte', F.input_file_name())\n",
    "            .writeStream.format('delta')\n",
    "            .outputMode('append')\n",
    "            .option(\"checkpointLocation\",   \"_checkpoint/bronze_planillas\")\n",
    "            .trigger(availableNow=True)\n",
    "            .option(\"path\", bronze_location).start()\n",
    "    ).awaitTermination()\n",
    "    \n",
    "load_raw_into_bronze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ae0e3fb-1dc2-49e9-a3ca-7b299f8a8c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_or_update_silver():\n",
    "\n",
    "    key_columns = ['nombre','apellido','cedula','salario','gasto','estado','fecha_de_inicio','institucion']\n",
    "\n",
    "    def deduplicate_bronze(bronze_table,key_columns=key_columns):\n",
    "        \n",
    "        return (bronze_table.orderBy(F.col('fecha_consulta'),ascending=False)\n",
    "                        .dropDuplicates( subset = key_columns )\n",
    "               )\n",
    "\n",
    "\n",
    "    \n",
    "    if  DeltaTable.isDeltaTable(spark,silver_location):\n",
    "        \n",
    "        sink_table = DeltaTable.forPath(spark,silver_location)\n",
    "        last_update = sink_table.history().select(F.max(F.col('timestamp'))).collect()[0][0]\n",
    "        \n",
    "        source_sdf = deduplicate_bronze(\n",
    "            spark.read.format('delta').option(\"readChangeFeed\", \"true\") \n",
    "                  .option(\"startingTimestamp\", str(last_update) )\n",
    "                         .load(bronze_location)\n",
    "                     )\n",
    "    \n",
    "        \n",
    "\n",
    "        (sink_table.alias('target')\n",
    "          .merge(source_sdf.alias('source'), ' AND '.join(list(map(lambda x: f'(source.{x} == target.{x})',key_columns))) )\n",
    "          .whenNotMatchedInsertAll() \n",
    "          .execute()\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        \n",
    "        source_sdf = deduplicate_bronze(spark.read.format('delta').load(bronze_location))\n",
    "\n",
    "        source_sdf.write.format('delta').partitionBy(['institucion']).save(silver_location)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "create_or_update_silver()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac0489f8-63ed-435b-8104-5d002dd4589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overwrite_agg_by_institucion():\n",
    "    ( spark.read.format('delta').load(silver_location).groupBy(['institucion','estado'])\\\n",
    "                                            .agg(\n",
    "                                                F.sum(F.col('salario')).alias('total_salarios'),\n",
    "                                                F.sum(F.col('gasto')).alias('total_gastos'), \n",
    "                                                F.countDistinct(F.col('cedula')).alias('total_personas')\n",
    "                                                    )\\\n",
    "                                                .withColumn('total_salario_+_gasto', F.col('total_salarios') + F.col('total_gastos'))\n",
    "                                                 .write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"True\").save(agg_by_institucion)\n",
    "    )\n",
    "create_overwrite_agg_by_institucion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6078d49-42d9-4529-86c9-42ddfc895a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overwrite_agg_by_institucion():\n",
    "    ( spark.read.format('delta').load(silver_location).groupBy(['cedula','estado'])\\\n",
    "                                            .agg(\n",
    "                                                F.sum(F.col('salario')).alias('total_salarios'),\n",
    "                                                F.sum(F.col('gasto')).alias('total_gastos'), \n",
    "                                                F.countDistinct(F.col('cedula')).alias('total_personas')\n",
    "                                                    )\\\n",
    "                                                .withColumn('total_salario_+_gasto', F.col('total_salarios') + F.col('total_gastos'))\n",
    "                                                 .write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"True\").save(agg_by_institucion)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e47c08-e63b-4a31-9098-00b6df886886",
   "metadata": {},
   "outputs": [],
   "source": [
    "  ( spark.read.format('delta').load(silver_location).groupBy(['cedula','estado'])\\\n",
    "                                            .agg(\n",
    "                                                F.sum(F.col('salario')).alias('total_salarios'),\n",
    "                                                F.sum(F.col('gasto')).alias('total_gastos'), \n",
    "                                                F.countDistinct(F.col('cedula')).alias('total_personas')\n",
    "                                                    )\\\n",
    "                                                .withColumn('total_salario_+_gasto', F.col('total_salarios') + F.col('total_gastos'))\n",
    "                                                 .write.format('delta').mode('overwrite').option(\"overwriteSchema\", \"True\").save(agg_by_institucion)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb31f7-3e42-4212-bcd8-341ecd13e877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dd4c8d4-995e-46bd-ab42-3455cdee5bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
