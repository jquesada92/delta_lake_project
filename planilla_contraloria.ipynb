{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc7f1ce-ca13-4491-b1e5-093181a73169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion Process\n",
      "Checking for updates\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u00002024-04-02 08:20:09\n",
      "no updates available\n"
     ]
    }
   ],
   "source": [
    "%run file_ingestion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc27ae7-4f3a-405b-a054-4aa34e0a737c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime as dt\n",
    "\n",
    "\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from config import *\n",
    "from delta import *\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark.sql(\"set spark.sql.streaming.schemaInference=true\")\n",
    "spark.sql(\"set SQLConf.ADAPTIVE_EXECUTION_ENABLED.key= true\")\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "spark.sql(\n",
    "    \"set spark.databricks.delta.changeDataFeed.timestampOutOfRange.enabled = true;\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e7d259d-f92f-43d8-b9aa-e776cf11c4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/12 16:09:43 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/12 16:09:43 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/12 16:09:44 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "24/04/12 16:09:44 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore hadoop@127.0.1.1\n",
      "24/04/12 16:09:45 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema} \")\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE EXTERNAL TABLE IF NOT EXISTS {bronze_table} \n",
    "  (nombre string,\n",
    "  apellido string,\n",
    "  cedula string,\n",
    "  cargo string,\n",
    "  salario double,\n",
    "  gasto double,\n",
    "  estado string,\n",
    "  fecha_de_inicio Timestamp,\n",
    "  fecha_actualizacion Timestamp,\n",
    "  fecha_consulta timestamp,\n",
    "  nombre_reporte string,\n",
    "  institucion string \n",
    "  \n",
    "  ) USING delta\n",
    "  LOCATION '{bronze_location}'\n",
    "  TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\")\n",
    "spark.sql(\n",
    "    f\"ALTER TABLE  {bronze_table}  SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5568f67a-d84a-4b21-b9fa-9713dda078f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[namespace: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" show databases; \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cdedb2b-3a13-4b47-b082-faef6a2a9621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/12 16:09:49 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_raw_into_bronze():\n",
    "\n",
    "    raw_data_sdf = spark.readStream.schema(\n",
    "        StructType(\n",
    "            [\n",
    "                StructField(\"Nombre\", StringType(), True),\n",
    "                StructField(\"Apellido\", StringType(), True),\n",
    "                StructField(\"Cédula\", StringType(), True),\n",
    "                StructField(\"Cargo\", StringType(), True),\n",
    "                StructField(\"Salario\", DoubleType(), True),\n",
    "                StructField(\"Gasto\", DoubleType(), True),\n",
    "                StructField(\"Estado\", StringType(), True),\n",
    "                StructField(\"Fecha de inicio\", StringType(), True),\n",
    "                StructField(\"Fecha Actualizacion\", StringType(), True),\n",
    "                StructField(\"Fecha Consulta\", StringType(), True),\n",
    "                StructField(\"Institucion\", StringType(), True),\n",
    "            ]\n",
    "        )\n",
    "    ).parquet(f\"{contraloria_source_folder}\")\n",
    "\n",
    "    columns = raw_data_sdf.columns\n",
    "\n",
    "    # Load Raw Data into Bronze\n",
    "\n",
    "    (\n",
    "        raw_data_sdf.select(\n",
    "            list(map(lambda x: F.col(x).alias(x.lower().replace(\" \", \"_\")), columns))\n",
    "        )\n",
    "        .withColumnRenamed(\"cédula\", \"cedula\")\n",
    "        .withColumn(\"fecha_de_inicio\", F.col(\"fecha_de_inicio\").cast(TimestampType()))\n",
    "        .withColumn(\n",
    "            \"fecha_actualizacion\", F.col(\"fecha_actualizacion\").cast(TimestampType())\n",
    "        )\n",
    "        .withColumn(\"fecha_consulta\", F.col(\"fecha_consulta\").cast(TimestampType()))\n",
    "        .withColumn(\"nombre_reporte\", F.input_file_name())\n",
    "        .writeStream.format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", \"contraloria/planillas/_checkpoint/bronze_planillas\")\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(bronze_table)\n",
    "    ).awaitTermination()\n",
    "\n",
    "\n",
    "load_raw_into_bronze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ae0e3fb-1dc2-49e9-a3ca-7b299f8a8c58",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create table or view `planilla_publicas_panama`.`silver` because it already exists.\nChoose a different name, drop or replace the existing object, or add the IF NOT EXISTS clause to tolerate pre-existing objects.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 67\u001b[0m\n\u001b[1;32m     57\u001b[0m         (\n\u001b[1;32m     58\u001b[0m             source_sdf\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m             \u001b[38;5;241m.\u001b[39mpartitionBy([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstitucion\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     60\u001b[0m             \u001b[38;5;241m.\u001b[39msaveAsTable(silver_table)\n\u001b[1;32m     61\u001b[0m         )\n\u001b[1;32m     62\u001b[0m         spark\u001b[38;5;241m.\u001b[39msql(\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALTER TABLE  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msilver_table\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m         )\n\u001b[0;32m---> 67\u001b[0m \u001b[43mcreate_or_update_silver\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 60\u001b[0m, in \u001b[0;36mcreate_or_update_silver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     source_sdf \u001b[38;5;241m=\u001b[39m deduplicate_bronze(spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtable(bronze_table))\n\u001b[1;32m     57\u001b[0m     (\n\u001b[1;32m     58\u001b[0m         \u001b[43msource_sdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minstitucion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 60\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43msilver_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     62\u001b[0m     spark\u001b[38;5;241m.\u001b[39msql(\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mALTER TABLE  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msilver_table\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1586\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_ALREADY_EXISTS] Cannot create table or view `planilla_publicas_panama`.`silver` because it already exists.\nChoose a different name, drop or replace the existing object, or add the IF NOT EXISTS clause to tolerate pre-existing objects."
     ]
    }
   ],
   "source": [
    "def create_or_update_silver():\n",
    "\n",
    "    key_columns = [\n",
    "        \"nombre\",\n",
    "        \"apellido\",\n",
    "        \"cedula\",\n",
    "        \"salario\",\n",
    "        \"gasto\",\n",
    "        \"estado\",\n",
    "        \"fecha_de_inicio\",\n",
    "        \"institucion\",\n",
    "        \"cargo\",\n",
    "    ]\n",
    "\n",
    "    def deduplicate_bronze(bronze_table, key_columns=key_columns):\n",
    "        window = Window.partitionBy(F.col(\"institucion\"))\n",
    "        return (\n",
    "            bronze_table.withColumn(\n",
    "                \"actualizado\", F.max(F.col(\"fecha_actualizacion\")).over(window)\n",
    "            )\n",
    "            .where(\"fecha_actualizacion == actualizado\")\n",
    "            .drop(\"actualizado\")\n",
    "            .orderBy(F.col(\"fecha_consulta\"), ascending=False)\n",
    "            .dropDuplicates(subset=key_columns)\n",
    "        )\n",
    "\n",
    "    if DeltaTable.isDeltaTable(spark, silver_location):\n",
    "\n",
    "        sink_table = DeltaTable.forPath(spark, silver_location)\n",
    "        last_update = (\n",
    "            sink_table.history().select(F.max(F.col(\"timestamp\"))).collect()[0][0]\n",
    "        )\n",
    "\n",
    "        source_sdf = deduplicate_bronze(\n",
    "            spark.read.format(\"delta\")\n",
    "            .option(\"readChangeFeed\", \"true\")\n",
    "            .option(\"startingTimestamp\", str(last_update))\n",
    "            .table(bronze_table)\n",
    "        )\n",
    "\n",
    "        (\n",
    "            sink_table.alias(\"target\")\n",
    "            .merge(\n",
    "                source_sdf.alias(\"source\"),\n",
    "                \" AND \".join(\n",
    "                    list(map(lambda x: f\"(source.{x} == target.{x})\", key_columns))\n",
    "                ),\n",
    "            )\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "\n",
    "    else:\n",
    "\n",
    "        source_sdf = deduplicate_bronze(spark.read.format(\"delta\").table(bronze_table))\n",
    "\n",
    "        (\n",
    "            source_sdf.write.format(\"delta\")\n",
    "            .partitionBy([\"institucion\"])\n",
    "            .saveAsTable(silver_table)\n",
    "        )\n",
    "        spark.sql(\n",
    "            f\"ALTER TABLE  {silver_table}  SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\"\n",
    "        )\n",
    "\n",
    "\n",
    "create_or_update_silver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61c8ff7-a6b8-4098-9194-8ad68aec799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy(F.col(\"institucion\"))\n",
    "silver_sdf = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(silver_location)\n",
    "    .withColumn(\"actualizado\", F.max(F.col(\"fecha_actualizacion\")).over(window))\n",
    "    .where(\"fecha_actualizacion == actualizado\")\n",
    "    .drop(\"actualizado\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f331274-3199-411e-b5c2-e85f2d3a23c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overwrite_agg_by_institucion(silver_sdf):\n",
    "\n",
    "    def agg_silver_by_institucion(sdf, group_by):\n",
    "        return (\n",
    "            silver_sdf.groupBy(group_by)\n",
    "            .agg(\n",
    "                F.sum(F.col(\"salario\")).alias(\"salarios_totales\"),\n",
    "                F.sum(F.col(\"gasto\")).alias(\"gastos_totales\"),\n",
    "                F.countDistinct(F.col(\"cedula\")).alias(\"total_personas\"),\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"salario_mas_gasto\", F.col(\"salarios_totales\") + F.col(\"gastos_totales\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "    agg_silver_by_institucion(silver_sdf, \"institucion\").withColumn(\n",
    "        \"estado\", F.lit(\"TODOS\")\n",
    "    ).unionByName(\n",
    "        agg_silver_by_institucion(silver_sdf, [\"institucion\", \"estado\"])\n",
    "    ).write.format(\n",
    "        \"delta\"\n",
    "    ).mode(\n",
    "        \"overwrite\"\n",
    "    ).option(\n",
    "        \"overwriteSchema\", \"True\"\n",
    "    ).save(\n",
    "        agg_by_institucion\n",
    "    )\n",
    "\n",
    "\n",
    "create_overwrite_agg_by_institucion(silver_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6174e2d8-4a3a-4429-a05e-5a003b946e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_report_agg_by_id(silver_sdf):\n",
    "\n",
    "    agg_by_cedula = (\n",
    "        silver_sdf.select(\n",
    "            \"*\",\n",
    "            F.to_json(\n",
    "                F.struct(\n",
    "                    F.col(\"institucion\"),\n",
    "                    F.col(\"estado\"),\n",
    "                    F.col(\"fecha_de_inicio\"),\n",
    "                    F.col(\"salario\"),\n",
    "                    F.col(\"gasto\"),\n",
    "                    F.col(\"fecha_consulta\"),\n",
    "                )\n",
    "            ).alias(\"detalle\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"id_trabajo\",\n",
    "            F.concat(\n",
    "                F.col(\"institucion\"),\n",
    "                F.col(\"estado\"),\n",
    "                F.col(\"fecha_de_inicio\"),\n",
    "                F.col(\"salario\"),\n",
    "            ),\n",
    "        )\n",
    "        .groupBy(\"cedula\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"id_trabajo\").alias(\"trabajos\"),\n",
    "            F.countDistinct(\"institucion\").alias(\"cantidad_de_instituciones\"),\n",
    "            F.sum(F.col(\"salario\")).alias(\"salario_total\"),\n",
    "            F.sum(F.col(\"gasto\")).alias(\"gastos_totales\"),\n",
    "            F.min(\"fecha_de_inicio\").alias(\"fecha_primer_trabajo_activo\"),\n",
    "            F.max(\"fecha_de_inicio\").alias(\"fecha_ultimo_trabajo_activo\"),\n",
    "            F.max(\"fecha_actualizacion\").alias(\"fecha_actualizacion\"),\n",
    "            F.collect_list(F.struct(F.col(\"detalle\"))).alias(\"detalle\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"salario_mas_gasto\", F.col(\"salario_total\") + F.col(\"gastos_totales\")\n",
    "        )\n",
    "        .write.format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"True\")\n",
    "        .save(agg_by_id)\n",
    "    )\n",
    "\n",
    "\n",
    "latest_report_agg_by_id(silver_sdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
