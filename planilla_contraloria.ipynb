{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc7f1ce-ca13-4491-b1e5-093181a73169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingestion Process\n",
      "Checking for updates\n",
      "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u00002024-04-02 08:20:09\n",
      "no updates available\n"
     ]
    }
   ],
   "source": [
    "%run file_ingestion.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc27ae7-4f3a-405b-a054-4aa34e0a737c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from config import *\n",
    "from delta import *\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark.sql(\"set spark.sql.streaming.schemaInference=true\")\n",
    "spark.sql(\"set SQLConf.ADAPTIVE_EXECUTION_ENABLED.key= true\")\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "spark.sql(\n",
    "    \"set spark.databricks.delta.changeDataFeed.timestampOutOfRange.enabled = true;\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e7d259d-f92f-43d8-b9aa-e776cf11c4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/09 21:53:06 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/09 21:53:06 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/04/09 21:53:09 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "24/04/09 21:53:09 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore hadoop@127.0.1.1\n",
      "24/04/09 21:53:09 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException\n",
      "24/04/09 21:53:09 WARN ObjectStore: Failed to get database planilla_publicas_panama, returning NoSuchObjectException\n",
      "24/04/09 21:53:09 WARN ObjectStore: Failed to get database planilla_publicas_panama, returning NoSuchObjectException\n",
      "24/04/09 21:53:09 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\n",
      "24/04/09 21:53:09 WARN ObjectStore: Failed to get database planilla_publicas_panama, returning NoSuchObjectException\n",
      "24/04/09 21:53:13 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`planilla_publicas_panama`.`bronze` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "24/04/09 21:53:13 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "24/04/09 21:53:13 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "24/04/09 21:53:13 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/04/09 21:53:13 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema} \")\n",
    "spark.sql(f\"\"\"\n",
    "  CREATE EXTERNAL TABLE IF NOT EXISTS {bronze_table} \n",
    "  (nombre string,\n",
    "  apellido string,\n",
    "  cedula string,\n",
    "  cargo string,\n",
    "  salario double,\n",
    "  gasto double,\n",
    "  estado string,\n",
    "  fecha_de_inicio Timestamp,\n",
    "  fecha_actualizacion Timestamp,\n",
    "  fecha_consulta timestamp,\n",
    "  nombre_reporte string,\n",
    "  institucion string \n",
    "  \n",
    "  ) USING delta\n",
    "  LOCATION '{bronze_location}'\n",
    "  TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\")\n",
    "spark.sql(\n",
    "    f\"ALTER TABLE  {bronze_table}  SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cdedb2b-3a13-4b47-b082-faef6a2a9621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/09 21:53:14 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_raw_into_bronze():\n",
    "\n",
    "    raw_data_sdf = spark.readStream.schema(\n",
    "        StructType(\n",
    "            [\n",
    "                StructField(\"Nombre\", StringType(), True),\n",
    "                StructField(\"Apellido\", StringType(), True),\n",
    "                StructField(\"Cédula\", StringType(), True),\n",
    "                StructField(\"Cargo\", StringType(), True),\n",
    "                StructField(\"Salario\", DoubleType(), True),\n",
    "                StructField(\"Gasto\", DoubleType(), True),\n",
    "                StructField(\"Estado\", StringType(), True),\n",
    "                StructField(\"Fecha de inicio\", StringType(), True),\n",
    "                StructField(\"Fecha Actualizacion\", StringType(), True),\n",
    "                StructField(\"Fecha Consulta\", StringType(), True),\n",
    "                StructField(\"Institucion\", StringType(), True),\n",
    "            ]\n",
    "        )\n",
    "    ).parquet(f\"{contraloria_source_folder}\")\n",
    "\n",
    "    columns = raw_data_sdf.columns\n",
    "\n",
    "    # Load Raw Data into Bronze\n",
    "\n",
    "    (\n",
    "        raw_data_sdf.select(\n",
    "            list(map(lambda x: F.col(x).alias(x.lower().replace(\" \", \"_\")), columns))\n",
    "        )\n",
    "        .withColumnRenamed(\"cédula\", \"cedula\")\n",
    "        .withColumn(\"fecha_de_inicio\", F.col(\"fecha_de_inicio\").cast(TimestampType()))\n",
    "        .withColumn(\n",
    "            \"fecha_actualizacion\", F.col(\"fecha_actualizacion\").cast(TimestampType())\n",
    "        )\n",
    "        .withColumn(\"fecha_consulta\", F.col(\"fecha_consulta\").cast(TimestampType()))\n",
    "        .withColumn(\"nombre_reporte\", F.input_file_name())\n",
    "        .writeStream.format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", \"contraloria/planillas/_checkpoint/bronze_planillas\")\n",
    "        .trigger(availableNow=True)\n",
    "        .toTable(bronze_table)\n",
    "    ).awaitTermination()\n",
    "\n",
    "\n",
    "load_raw_into_bronze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ae0e3fb-1dc2-49e9-a3ca-7b299f8a8c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/09 21:53:16 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`planilla_publicas_panama`.`silver` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    }
   ],
   "source": [
    "def create_or_update_silver():\n",
    "\n",
    "    key_columns = [\n",
    "        \"nombre\",\n",
    "        \"apellido\",\n",
    "        \"cedula\",\n",
    "        \"salario\",\n",
    "        \"gasto\",\n",
    "        \"estado\",\n",
    "        \"fecha_de_inicio\",\n",
    "        \"institucion\",\n",
    "        \"cargo\",\n",
    "    ]\n",
    "\n",
    "    def deduplicate_bronze(bronze_table, key_columns=key_columns):\n",
    "        window = Window.partitionBy(F.col(\"institucion\"))\n",
    "        return (\n",
    "            bronze_table.withColumn(\n",
    "                \"actualizado\", F.max(F.col(\"fecha_actualizacion\")).over(window)\n",
    "            )\n",
    "            .where(\"fecha_actualizacion == actualizado\")\n",
    "            .drop(\"actualizado\")\n",
    "            .orderBy(F.col(\"fecha_consulta\"), ascending=False)\n",
    "            .dropDuplicates(subset=key_columns)\n",
    "        )\n",
    "\n",
    "    if DeltaTable.isDeltaTable(spark, silver_location):\n",
    "\n",
    "        sink_table = DeltaTable.forPath(spark, silver_location)\n",
    "        last_update = (\n",
    "            sink_table.history().select(F.max(F.col(\"timestamp\"))).collect()[0][0]\n",
    "        )\n",
    "\n",
    "        source_sdf = deduplicate_bronze(\n",
    "            spark.read.format(\"delta\")\n",
    "            .option(\"readChangeFeed\", \"true\")\n",
    "            .option(\"startingTimestamp\", str(last_update))\n",
    "            .table(bronze_table)\n",
    "        )\n",
    "\n",
    "        (\n",
    "            sink_table.alias(\"target\")\n",
    "            .merge(\n",
    "                source_sdf.alias(\"source\"),\n",
    "                \" AND \".join(\n",
    "                    list(map(lambda x: f\"(source.{x} == target.{x})\", key_columns))\n",
    "                ),\n",
    "            )\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute()\n",
    "        )\n",
    "\n",
    "    else:\n",
    "\n",
    "        source_sdf = deduplicate_bronze(spark.read.format(\"delta\").table(bronze_table))\n",
    "\n",
    "        (\n",
    "            source_sdf.write.format(\"delta\")\n",
    "            .partitionBy([\"institucion\"])\n",
    "            .saveAsTable(silver_table)\n",
    "        )\n",
    "        spark.sql(\n",
    "            f\"ALTER TABLE  {silver_table}  SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\"\n",
    "        )\n",
    "\n",
    "\n",
    "create_or_update_silver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f61c8ff7-a6b8-4098-9194-8ad68aec799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy(F.col(\"institucion\"))\n",
    "silver_sdf = (\n",
    "    spark.read.format(\"delta\")\n",
    "    .load(silver_location)\n",
    "    .withColumn(\"actualizado\", F.max(F.col(\"fecha_actualizacion\")).over(window))\n",
    "    .where(\"fecha_actualizacion == actualizado\")\n",
    "    .drop(\"actualizado\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f331274-3199-411e-b5c2-e85f2d3a23c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_overwrite_agg_by_institucion(silver_sdf):\n",
    "\n",
    "    def agg_silver_by_institucion(sdf, group_by):\n",
    "        return (\n",
    "            silver_sdf.groupBy(group_by)\n",
    "            .agg(\n",
    "                F.sum(F.col(\"salario\")).alias(\"salarios_totales\"),\n",
    "                F.sum(F.col(\"gasto\")).alias(\"gastos_totales\"),\n",
    "                F.countDistinct(F.col(\"cedula\")).alias(\"total_personas\"),\n",
    "            )\n",
    "            .withColumn(\n",
    "                \"salario_mas_gasto\", F.col(\"salarios_totales\") + F.col(\"gastos_totales\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "    agg_silver_by_institucion(silver_sdf, \"institucion\").withColumn(\n",
    "        \"estado\", F.lit(\"TODOS\")\n",
    "    ).unionByName(\n",
    "        agg_silver_by_institucion(silver_sdf, [\"institucion\", \"estado\"])\n",
    "    ).write.format(\n",
    "        \"delta\"\n",
    "    ).mode(\n",
    "        \"overwrite\"\n",
    "    ).option(\n",
    "        \"overwriteSchema\", \"True\"\n",
    "    ).save(\n",
    "        agg_by_institucion\n",
    "    )\n",
    "\n",
    "\n",
    "create_overwrite_agg_by_institucion(silver_sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6174e2d8-4a3a-4429-a05e-5a003b946e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_report_agg_by_id(silver_sdf):\n",
    "\n",
    "    agg_by_cedula = (\n",
    "        silver_sdf.select(\n",
    "            \"*\",\n",
    "            F.to_json(\n",
    "                F.struct(\n",
    "                    F.col(\"institucion\"),\n",
    "                    F.col(\"estado\"),\n",
    "                    F.col(\"fecha_de_inicio\"),\n",
    "                    F.col(\"salario\"),\n",
    "                    F.col(\"gasto\"),\n",
    "                    F.col(\"fecha_consulta\"),\n",
    "                )\n",
    "            ).alias(\"detalle\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"id_trabajo\",\n",
    "            F.concat(\n",
    "                F.col(\"institucion\"),\n",
    "                F.col(\"estado\"),\n",
    "                F.col(\"fecha_de_inicio\"),\n",
    "                F.col(\"salario\"),\n",
    "            ),\n",
    "        )\n",
    "        .groupBy(\"cedula\")\n",
    "        .agg(\n",
    "            F.countDistinct(\"id_trabajo\").alias(\"trabajos\"),\n",
    "            F.countDistinct(\"institucion\").alias(\"cantidad_de_instituciones\"),\n",
    "            F.sum(F.col(\"salario\")).alias(\"salario_total\"),\n",
    "            F.sum(F.col(\"gasto\")).alias(\"gastos_totales\"),\n",
    "            F.min(\"fecha_de_inicio\").alias(\"fecha_primer_trabajo_activo\"),\n",
    "            F.max(\"fecha_de_inicio\").alias(\"fecha_ultimo_trabajo_activo\"),\n",
    "            F.max(\"fecha_actualizacion\").alias(\"fecha_actualizacion\"),\n",
    "            F.collect_list(F.struct(F.col(\"detalle\"))).alias(\"detalle\"),\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"salario_mas_gasto\", F.col(\"salario_total\") + F.col(\"gastos_totales\")\n",
    "        )\n",
    "        .write.format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"True\")\n",
    "        .save(agg_by_id)\n",
    "    )\n",
    "\n",
    "\n",
    "latest_report_agg_by_id(silver_sdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
